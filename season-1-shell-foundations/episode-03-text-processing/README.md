# Episode 03: Text Processing Masters

```
ОПЕРАЦИЯ: KERNEL SHADOWS
СЕЗОН: 1 — Shell & Foundations
ЭПИЗОД: 03 — Text Processing Masters
ЛОКАЦИЯ: 🇷🇺 Новосибирск, Россия (НГУ + Академгородок)
ДНИ ОПЕРАЦИИ: 5-6 (из 60)
СЛОЖНОСТЬ: ⭐⭐⭐☆☆
ВРЕМЯ: 2-2.5 часа
```

---

## 🎬 Пролог: Атака

### 📅 День 5 (05 октября 2025) — 03:52

Телефон Макса взрывается звонком. Неизвестный номер (Москва).

Голос женский, жёсткий, без паники. Но слышно напряжение.

> **???:**
> *"Максим Соколов? Анна Ковалева. Forensics team. Нас атаковали. Вчера в 03:47."*

> **Анна (видеозвонок):**
> *"DDoS на сервер Виктора. 4,500+ requests/sec. Сервер выдержал, но логи огромные — 100MB+."*

На экране мониторы с графиками:

```
[04/Oct/2025:03:47:23 +0000] - 2,847 requests/sec
[04/Oct/2025:03:47:24 +0000] - 3,124 requests/sec
[04/Oct/2025:03:47:25 +0000] - 4,582 requests/sec ⚠️
SERVER OVERLOAD WARNING
```

> **Анна:**
> *"Мне нужны IP атакующих. Найди их в логах. Срочно. Отправлю на email: access.log и базу известных угроз."*

Пауза.

> **Анна:**
> *"Крылов уже знает о нас. Это его почерк. Найди доказательства."*

Звонок обрывается.

---

## 🔧 Подготовка

Перейди в директорию эпизода:

```bash
cd ~/kernel-shadows/season-1-shell-foundations/episode-03-text-processing
```

Проверь артефакты:

```bash
ls -lh artifacts/
# access.log — 4400 строк логов веб-сервера (атака 03:47-04:00)
# suspicious_ips.txt — база известных угроз от Anna
# report_template.txt — шаблон отчёта
```

> 💡 **LILITH:** *"Anna дала тебе логи. 4,400 строк. Deadline — через 2 часа. Найди атакующих. Используй инструменты, не глаза."*

---

## Цикл 1: Зачем text processing? (10-12 минут)

### 🎬 Сюжет: Океан данных

**09:00 — Квартира Макса, Академгородок**

Макс открывает email. Вложения: `access.log` (100MB+ симуляция), `suspicious_ips.txt`.

Открывает лог. Экран заливает текст:

```
185.220.101.47 - - [04/Oct/2025:03:47:23 +0000] "GET /admin HTTP/1.1" 404 0 "-" "nmap NSE"
91.234.56.78 - - [04/Oct/2025:03:47:23 +0000] "POST /login HTTP/1.1" 401 128 "-" "curl/7.58.0"
45.155.205.67 - - [04/Oct/2025:03:47:24 +0000] "GET /wp-admin HTTP/1.1" 403 0 "-" "sqlmap/1.5.2"
...
(4,400 строк)
```

Макс в растерянности. **Как анализировать это вручную?**

Звонок Дмитрия:

> **Дмитрий (видео):**
> *"Макс, знаю про атаку. Есть человек в Новосибирске. Ольга Петрова, data scientist из Yandex. Эксперт по big data. Встретишься в НГУ. Адрес отправлю."*

---

### 📚 Теория: Проблема больших данных

#### Метафора: Океан данных

**Представь:** Ты ищешь затонувший корабль в океане.

- **Ручной поиск** = нырять с маской, осматривать каждый квадратный метр
- **Text processing tools** = подводная лодка с сонаром (сканирует миллионы точек за секунды)

**В нашем случае:**
- Логи = океан данных
- grep/awk/sed = сонар (находит patterns)
- Ты = капитан подводной лодки (управляешь инструментами)

#### Математика ручного анализа

**Задача:** Найти IP атакующих в 4,400 строках логов.

**Ручной подход:**
```
1 строка = ~10 секунд (прочитать, понять, записать IP)
4,400 строк × 10 секунд = 44,000 секунд = 12.2 часа
```

**12 часов непрерывной работы!** 😱

**Подход с инструментами:**
```bash
grep "03:47" access.log | awk '{print $1}' | sort | uniq -c | sort -nr | head -10
```

**2 секунды.** ⚡

**Разница:** 12 часов vs 2 секунды = **21,600x быстрее!**

```
┌─────────────────────────────────────────────────────────────┐
│ LILITH:                                                       │
│                                                               │
│ "Text processing — это не магия. Это знание инструментов.    │
│                                                               │
│  Логи = океан. grep = сонар. awk = робот-сортировщик.        │
│  sed = текстовый хирург. pipes = конвейер.                   │
│                                                               │
│  Ты не читаешь 4,400 строк. Ты управляешь инструментами     │
│  которые делают это за тебя.                                 │
│                                                               │
│  Время учиться."                                             │
└─────────────────────────────────────────────────────────────┘
```

---

### 💻 Практика: Первый взгляд на логи (5 минут)

**Команды разведки** — быстрый анализ файла:

```bash
cd artifacts/

# Сколько строк?
wc -l access.log
# Вывод: 4400 access.log

# Первые 5 строк (начало файла)
head -5 access.log

# Последние 5 строк (конец файла)
tail -5 access.log

# Размер файла
ls -lh access.log
# ~400KB (реальные логи могут быть гигабайты!)

# Постраничный просмотр (для изучения формата)
less access.log
# Навигация: пробел = вниз, b = вверх, q = выход
```

**Формат Apache Combined Log:**

```
IP - - [timestamp] "METHOD /path HTTP/1.1" status size "referer" "user-agent"
│   │ │  │          │                       │      │     │         │
│   │ │  │          │                       │      │     │         └─ Программа клиента
│   │ │  │          │                       │      │     └─ Откуда пришёл
│   │ │  │          │                       │      └─ Размер ответа (bytes)
│   │ │  │          │                       └─ HTTP status code
│   │ │  │          └─ Запрос (метод + путь)
│   │ │  └─ Дата/время (04/Oct/2025:03:47:23 +0000)
│   │ └─ Аутентификация (- = нет)
│   └─ Remote logname (- = нет)
└─ IP адрес клиента
```

**Пример строки:**
```
185.220.101.47 - - [04/Oct/2025:03:47:23 +0000] "GET /admin HTTP/1.1" 404 0 "-" "nmap NSE"
```

**Что видим:**
- **IP:** 185.220.101.47 (кто?)
- **Время:** 03:47:23 (когда? — во время атаки!)
- **Запрос:** GET /admin (что искал?)
- **Статус:** 404 (не найдено)
- **User-Agent:** "nmap NSE" (сканер сети! 🚨)

**Задание:**
1. Откройте `access.log` через `less`
2. Найдите строки с timestamp `03:47` (атака)
3. Обратите внимание на повторяющиеся IP
4. Посмотрите User-Agent (последнее поле в кавычках)

---

### 🤔 Проверка понимания

**LILITH:** *"Быстрая проверка. Математика эффективности."*

**Вопрос:** Реальные production логи — 10,000,000 строк (10 миллионов). Вручную = 10 секунд/строка. Сколько времени уйдёт на анализ?

a) 27 часов
b) 277 часов (11.5 дней)
c) 2,777 часов (115 дней)
d) 27,777 часов (3.2 года)

<details>
<summary>🤔 Думай 30 секунд</summary>

**Ответ: d) 27,777 часов (3.2 года непрерывной работы)**

**Расчёт:**
```
10,000,000 строк × 10 секунд = 100,000,000 секунд
100,000,000 / 3600 = 27,777 часов
27,777 / 24 = 1,157 дней = 3.17 года
```

**С инструментами:**
```bash
grep "pattern" 10M_lines.log | awk '{print $1}' | sort | uniq -c | sort -nr | head -10
```

**Время:** ~5-10 секунд на современном сервере.

**Разница:** 3.2 года vs 10 секунд = **~100 миллионов раз быстрее!**

**LILITH:** *"Видишь теперь зачем существуют grep, awk, sed? Без них Linux администрирование невозможно. Это не просто 'удобно'. Это критично."*

</details>

---

## Цикл 2: grep — детектив с лупой (12-15 минут)

### 🎬 Сюжет: Инструмент Ольги

**11:00 — НГУ (Новосибирский Государственный Университет)**

Лаборатория информационных технологий. Белые доски с формулами. Запах кофе.

За столом — женщина ~29 лет, ноутбук с наклейками Yandex, Python.

> **???:**
> *"Максим? Ольга Петрова. Дмитрий просил помочь с логами."*

Макс показывает `access.log` (4,400 строк).

> **Ольга:**
> *"4,400? Это не много. grep справится за секунду. Смотри."*

Она открывает терминал. Быстрые пальцы:

```bash
grep "03:47" access.log | wc -l
```

**Вывод:** `1,247`

> **Ольга:**
> *"1,247 запросов за минуту 03:47. Атака."*

Макс в шоке. **Одна команда. Секунда.**

> **Ольга:**
> *"grep — это детектив с лупой. Находит patterns в тексте. Регулярные выражения. Фильтрация. Покажу."*

```
┌─────────────────────────────────────────────────────────────┐
│ LILITH:                                                       │
│                                                               │
│ "grep = Global Regular Expression Print                      │
│                                                               │
│  Представь детектива с лупой на месте преступления.          │
│  Миллион улик (строк). Он ищет конкретный pattern:           │
│  'отпечаток пальца', 'номер машины', 'timestamp'.            │
│                                                               │
│  grep делает это за миллисекунды. Находит ВСЕ совпадения.    │
│  Остальное отбрасывает. Сигнал из шума."                     │
└─────────────────────────────────────────────────────────────┘
```

---

### 📚 Теория: grep — поиск patterns

#### Базовый синтаксис

```bash
grep "pattern" file.txt
```

**Что делает:**
- Ищет `pattern` в каждой строке `file.txt`
- Выводит **только строки** которые содержат pattern
- Остальное игнорирует

**Пример:**

```bash
grep "ERROR" app.log
# Выведет только строки содержащие "ERROR"
```

#### Важные флаги

| Флаг | Действие | Пример |
|------|----------|--------|
| `-i` | Case-insensitive (игнорировать регистр) | `grep -i "error" log` |
| `-v` | Инверт (строки БЕЗ pattern) | `grep -v "200" log` |
| `-c` | Count (только количество совпадений) | `grep -c "ERROR" log` |
| `-n` | Номера строк | `grep -n "pattern" log` |
| `-A N` | Показать N строк ПОСЛЕ совпадения | `grep -A 5 "ERROR" log` |
| `-B N` | Показать N строк ДО совпадения | `grep -B 5 "ERROR" log` |
| `-C N` | Показать N строк ДО и ПОСЛЕ | `grep -C 3 "ERROR" log` |
| `-E` | Extended regex (ИЛИ, диапазоны) | `grep -E "ERROR\|WARN" log` |
| `-o` | Вывести только matching часть | `grep -o "[0-9]\+" log` |

#### Примеры использования

**Найти строки с timestamp атаки:**
```bash
grep "03:47" access.log
```

**Посчитать сколько:**
```bash
grep "03:47" access.log | wc -l
# или
grep -c "03:47" access.log
```

**Найти строки БЕЗ статуса 200 (ошибки):**
```bash
grep -v " 200 " access.log
```

**Case-insensitive поиск:**
```bash
grep -i "error" app.log
# Найдёт: ERROR, Error, error, ErRoR
```

**С контекстом (5 строк до и после):**
```bash
grep -C 5 "CRITICAL" app.log
```

#### Regular Expressions (regex) — базовый уровень

**grep поддерживает regex для сложных patterns:**

| Pattern | Значение | Пример |
|---------|----------|--------|
| `.` | Любой символ | `gr.p` → grep, gr3p, gr@p |
| `*` | 0 или больше предыдущего | `12*3` → 13, 123, 1223 |
| `+` | 1 или больше (-E флаг) | `12+3` → 123, 1223 |
| `^` | Начало строки | `^ERROR` → строки начинающиеся с ERROR |
| `$` | Конец строки | `ERROR$` → строки заканчивающиеся ERROR |
| `[abc]` | Один из символов | `[0-9]` → любая цифра |
| `[^abc]` | НЕ эти символы | `[^0-9]` → не цифра |
| `\|` | ИЛИ (-E флаг) | `ERROR\|WARN` → ERROR или WARN |

**Примеры regex:**

```bash
# Найти IP адреса (упрощённо)
grep -E "[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+" access.log

# Найти строки с ERROR или WARNING
grep -E "ERROR|WARNING" app.log

# Найти строки начинающиеся с цифры
grep "^[0-9]" file.txt

# Найти строки с HTTP 4xx или 5xx ошибками
grep -E " [45][0-9]{2} " access.log

# Найти timestamp диапазон (03:00 - 04:59)
grep -E "0[34]:[0-9]{2}" access.log
```

> **LILITH:** *"Regex — язык patterns. Мощный, но уродливый. Как SQL или assembly. Учи постепенно. Начни с простого: grep 'text', потом добавляй ^ $ [], потом |. Не пытайся запомнить всё сразу."*

---

### 💻 Практика: Фильтруем логи атаки (7 минут)

**Задание 1:** Найди все запросы во время атаки (03:47-03:49)

```bash
cd artifacts/

# Вариант 1: Только 03:47
grep "03:47" access.log | head -10

# Вариант 2: Диапазон 03:47-03:49
grep -E "03:4[789]" access.log | head -10

# Посчитай сколько
grep -c "03:47" access.log
```

**Задание 2:** Найди только ошибки 4xx и 5xx

```bash
# Все строки с кодами 400-599
grep -E " [45][0-9]{2} " access.log | head -10

# Только 404 (Not Found)
grep " 404 " access.log | wc -l

# Только 403 (Forbidden)
grep " 403 " access.log | wc -l
```

**Задание 3:** Сохрани логи атаки в файл

```bash
grep "03:47" access.log > attack_window.txt

# Проверь
wc -l attack_window.txt
head -5 attack_window.txt
```

**Задание 4:** Найди строки содержащие "nmap" (сканер!)

```bash
grep -i "nmap" access.log | wc -l
```

---

### 🤔 Проверка понимания

**LILITH:** *"grep — основа. Без него дальше не пойдём. Проверка."*

**Вопрос 1:** Что делает `grep -v "200" access.log`?

<details>
<summary>Ответ</summary>

**Выводит строки которые НЕ содержат "200"**

- `-v` = invert match (инверт)
- Показывает всё КРОМЕ успешных запросов (статус 200)
- Полезно для поиска ошибок

**Пример:**
```bash
# Все запросы кроме успешных
grep -v " 200 " access.log

# Только проблемные запросы остаются:
# 404, 403, 500, 503, etc.
```

**LILITH:** *"`-v` — твой фильтр шума. Когда нужно найти проблемы, отбрось успех. Останется только интересное."*

</details>

**Вопрос 2:** В чём разница между `grep "error"` и `grep -i "error"`?

<details>
<summary>Ответ</summary>

**Регистр символов:**

```bash
# grep "error" — точное совпадение (case-sensitive)
grep "error" log
# Найдёт: error
# НЕ найдёт: ERROR, Error, ErRoR

# grep -i "error" — игнорирует регистр
grep -i "error" log
# Найдёт: error, ERROR, Error, ErRoR, eRrOr
```

**Когда использовать:**
- **Без `-i`:** Когда нужно точное совпадение
- **С `-i`:** Когда регистр не важен (логи часто непредсказуемы)

**LILITH:** *"В логах регистр часто хаотичен. Одна программа пишет ERROR, другая Error. Используй `-i` если не уверен."*

</details>

**Вопрос 3:** Что делает `grep -E "03:4[789]"`?

<details>
<summary>Ответ</summary>

**Regex pattern для диапазона:**

- `-E` = extended regex
- `[789]` = один из символов: 7, 8, 9
- `03:4[789]` = **03:47** или **03:48** или **03:49**

**Найдёт все timestamps:**
- 03:47:00
- 03:47:23
- 03:48:15
- 03:49:59

**НЕ найдёт:**
- 03:46:xx
- 03:50:xx
- 04:47:xx

**Эквивалент без regex:**
```bash
grep "03:47" access.log
grep "03:48" access.log
grep "03:49" access.log
```

Но через regex — **одна команда вместо трёх!**

**LILITH:** *"Regex экономит команды. Три grep → один grep с pattern. Эффективность."*

</details>

---

## Цикл 3: pipes — конвейер данных (10-12 минут)

### 🎬 Сюжет: Конвейер Ольги

> **Ольга:**
> *"grep находит строки. Но тебе нужны IP адреса. Как их извлечь?"*

Макс думает: *"Regex для IP? Сложно..."*

> **Ольга:**
> *"Нет. Проще. Используй конвейер. Pipes."*

Она печатает:

```bash
grep "03:47" access.log | awk '{print $1}' | head -10
```

**Вывод — только IP адреса:**
```
185.220.101.47
91.234.56.78
45.155.205.67
...
```

> **Ольга:**
> *"Видишь? Две команды через pipe. Первая находит строки. Вторая извлекает первое поле."*

```
┌─────────────────────────────────────────────────────────────┐
│ LILITH:                                                       │
│                                                               │
│ "Pipe (|) — конвейер данных. Водопровод. Производственная    │
│  линия.                                                       │
│                                                               │
│  Представь фабрику:                                           │
│  1. Сырьё (логи) → машина 1 (grep) → отфильтрованные детали  │
│  2. Детали → машина 2 (awk) → извлечённые поля               │
│  3. Поля → машина 3 (sort) → отсортированный результат       │
│                                                               │
│  Каждая машина делает ОДНУ вещь хорошо.                      │
│  Pipe соединяет их в конвейер. Unix философия."              │
└─────────────────────────────────────────────────────────────┘
```

---

### 📚 Теория: Pipes — соединение команд

#### Концепция

**Pipe (`|`)** — оператор перенаправления вывода одной команды во вход другой.

```bash
command1 | command2 | command3
```

**Визуализация:**

```
┌─────────────────────────────────────────────────────────────┐
│              КОНВЕЙЕР ОБРАБОТКИ ДАННЫХ                        │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│  [Файл]                                                       │
│     │                                                         │
│     ▼                                                         │
│  ┌───────┐                                                    │
│  │ grep  │ ──> Фильтрует строки                              │
│  └───────┘                                                    │
│     │ (только нужные строки)                                 │
│     ▼                                                         │
│  ┌───────┐                                                    │
│  │  awk  │ ──> Извлекает колонки                             │
│  └───────┘                                                    │
│     │ (только нужные поля)                                   │
│     ▼                                                         │
│  ┌───────┐                                                    │
│  │ sort  │ ──> Сортирует                                     │
│  └───────┘                                                    │
│     │ (упорядоченный вывод)                                  │
│     ▼                                                         │
│  ┌───────┐                                                    │
│  │ uniq  │ ──> Убирает дубликаты + считает                   │
│  └───────┘                                                    │
│     │                                                         │
│     ▼                                                         │
│  [Результат]                                                  │
│                                                               │
└─────────────────────────────────────────────────────────────┘
```

**Каждая команда:**
- Читает из stdin (standard input)
- Обрабатывает данные
- Пишет в stdout (standard output)
- Следующая команда читает этот stdout

**Эффект:** Сложная обработка через цепочку простых инструментов.

#### Unix философия: "Do One Thing Well"

**Вместо одного монолитного инструмента** → много специализированных:

- `grep` — фильтрует строки ✅
- `awk` — извлекает колонки ✅
- `sort` — сортирует ✅
- `uniq` — находит уникальные ✅
- `head/tail` — берёт N строк ✅
- `wc` — считает ✅

**Pipe соединяет их в мощные комбинации!**

> **LILITH:** *"Unix создан в 1970х. Pipe — гениальное решение. Одна команда = одна задача. Комбинируй как LEGO. 50 лет спустя — всё ещё работает. Это не баг, это feature."*

---

### 💻 Практика: Строим конвейеры (7 минут)

**Задание 1:** Посчитай уникальные IP в логах

```bash
cd artifacts/

# Конвейер из 3 команд:
awk '{print $1}' access.log | sort -u | wc -l

# Разбор:
# 1. awk '{print $1}' — извлечь первое поле (IP)
# 2. sort -u — отсортировать и оставить уникальные
# 3. wc -l — посчитать строки
```

**Задание 2:** TOP-5 самых частых IP

```bash
# Конвейер из 5 команд:
awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -5

# Разбор:
# 1. awk '{print $1}' — извлечь IP
# 2. sort — отсортировать (для uniq)
# 3. uniq -c — подсчитать повторения
# 4. sort -nr — сортировать по числу (reverse numeric)
# 5. head -5 — взять TOP-5

# Вывод:
#     523 185.220.101.47
#     156 91.234.56.78
#     134 45.155.205.67
#     ...
```

**Задание 3:** Найди сколько запросов с каждым HTTP статусом

```bash
awk '{print $9}' access.log | sort | uniq -c | sort -nr

# Вывод:
#   3200 200
#    800 404
#    250 403
#    100 500
#     50 503
```

**Задание 4:** Построй свой конвейер

**Цель:** Найти TOP-3 IP атаковавших в 03:47

```bash
grep "03:47" access.log | awk '{print $1}' | sort | uniq -c | sort -nr | head -3
```

---

### 🤔 Проверка понимания

**LILITH:** *"Pipes — основа мощи Unix. Проверка."*

**Вопрос 1:** Что делает каждая команда в этом конвейере?

```bash
cat file.txt | grep "ERROR" | wc -l
```

<details>
<summary>Ответ</summary>

**Пошаговый разбор:**

1. **`cat file.txt`** — читает файл, выводит в stdout
2. **`|`** — перенаправляет вывод в следующую команду
3. **`grep "ERROR"`** — фильтрует, оставляет только строки с "ERROR"
4. **`|`** — перенаправляет отфильтрованные строки
5. **`wc -l`** — считает количество строк

**Результат:** Количество строк содержащих "ERROR" в файле.

**Упрощение:**
```bash
# Можно без cat:
grep "ERROR" file.txt | wc -l

# Или ещё проще:
grep -c "ERROR" file.txt
```

**LILITH:** *"Несколько путей к одному результату. Это Unix. Выбирай читаемый или быстрый — зависит от ситуации."*

</details>

**Вопрос 2:** Почему `uniq` требует `sort` перед собой?

<details>
<summary>Ответ</summary>

**`uniq` находит дубликаты только на СОСЕДНИХ строках!**

**Пример без sort:**
```bash
echo -e "apple\nbanana\napple" | uniq
# Вывод:
# apple
# banana
# apple   ← НЕ удалён! (не рядом с первым)
```

**С sort:**
```bash
echo -e "apple\nbanana\napple" | sort | uniq
# Вывод:
# apple   ← один раз
# banana
```

**Как работает `uniq`:**
```
Входные строки:
apple    ← запомнил
apple    ← дубликат соседа → пропустить
banana   ← новая → вывести
apple    ← не сосед с первым → вывести (!)
```

**Поэтому нужен sort:**
```
После sort:
apple
apple    ← все вместе!
apple
banana

После uniq:
apple    ← один раз
banana
```

**"Aha!" момент:**
```bash
# ❌ Неправильно
uniq file.txt       # Может оставить дубликаты!

# ✅ Правильно
sort file.txt | uniq
```

**LILITH:** *"`uniq` ленивый. Смотрит только на соседа. Не сканирует весь список. Поэтому быстрый. Но требует предварительную сортировку. sort + uniq — неразлучная пара."*

</details>

**Вопрос 3:** Как прочитать этот конвейер?

```bash
grep "03:47" access.log | awk '{print $1}' | sort | uniq -c | sort -nr | head -10
```

<details>
<summary>Ответ</summary>

**Читаем слева направо (конвейер):**

1. **`grep "03:47" access.log`**
   - Фильтр: только строки с timestamp 03:47

2. **`| awk '{print $1}'`**
   - Извлечение: только первое поле (IP адрес)

3. **`| sort`**
   - Сортировка: IP в алфавитном порядке

4. **`| uniq -c`**
   - Подсчёт: сколько раз встречается каждый IP

5. **`| sort -nr`**
   - Сортировка: по количеству, от большего к меньшему

6. **`| head -10`**
   - Ограничение: только TOP-10

**Результат:**
```
    523 185.220.101.47
    156 91.234.56.78
    134 45.155.205.67
    ...
```

**TOP-10 атакующих IP за период 03:47!**

**В одну строку. Без программирования. Без циклов. Просто инструменты.**

**LILITH:** *"Это мощь pipes. Шесть простых инструментов → сложный анализ. Ольга показала тебе это за 2 секунды. Запомни этот pattern. Будешь использовать постоянно."*

</details>

---

---

## Цикл 4: awk — швейцарский нож (12-15 минут)

### 🎬 Сюжет: Инструмент хирурга

**12:15 — Кафетерий НГУ**

Ольга заказывает эспрессо. Макс — американо.

> **Макс:**
> *"Ты использовала `awk '{print $1}'` для извлечения IP. Что такое $1? Почему не regex?"*

> **Ольга (улыбка):**
> *"awk — швейцарский нож text processing. Видит строку как таблицу. Столбцы разделены пробелами. $1 = первый столбец, $2 = второй..."*

Она рисует на салфетке:

```
Лог-строка (пробелы = разделители):

185.220.101.47 - - [04/Oct/2025:03:47:23 +0000] "GET /admin" 404 0
│              │ │ │                            │              │   │
$1             $2 $3 $4                          $6            $9  $10

awk '{print $1}'     → IP адрес
awk '{print $9}'     → HTTP status
awk '{print $6}'     → HTTP метод (с кавычками)
```

> **Ольга:**
> *"awk читает построчно. Автоматически разбивает на поля. Ты просто говоришь: дай мне поле номер N."*

```
┌─────────────────────────────────────────────────────────────┐
│ LILITH:                                                       │
│                                                               │
│ "awk = швейцарский нож для структурированного текста.        │
│                                                               │
│  Представь таблицу Excel без границ. Только пробелы.         │
│  awk видит каждую строку как ROW с COLUMNS.                  │
│                                                               │
│  $1 $2 $3 = первые три столбца                               │
│  $0 = вся строка                                             │
│  $NF = последний столбец (Number of Fields)                  │
│                                                               │
│  Regex для извлечения колонок? Убийство. awk? Элегантность." │
└─────────────────────────────────────────────────────────────┘
```

---

### 📚 Теория: awk — обработка полей

#### Базовый синтаксис

```bash
awk '{print $N}' file.txt
```

**Что делает:**
- Читает файл построчно
- Разбивает каждую строку на **поля** (по умолчанию разделитель = пробел)
- `$1` = первое поле, `$2` = второе, и т.д.
- `$0` = вся строка
- `$NF` = последнее поле (Number of Fields)

#### Автоматическое разбиение на поля

**Пример строки:**
```
185.220.101.47 - - [04/Oct/2025:03:47:23] "GET /admin" 404
```

**awk видит это как:**

```
┌────────────────────────────────────────────────────────────┐
│  Поля (разделитель = пробел или tab):                      │
├────────────────────────────────────────────────────────────┤
│                                                             │
│  $1  = 185.220.101.47                                      │
│  $2  = -                                                   │
│  $3  = -                                                   │
│  $4  = [04/Oct/2025:03:47:23]                              │
│  $5  = "GET                                                │
│  $6  = /admin"                                             │
│  $7  = 404                                                 │
│                                                             │
│  $0  = (вся строка целиком)                                │
│  $NF = 404 (последнее поле, Field #7)                      │
│  NF  = 7 (количество полей)                                │
│                                                             │
└────────────────────────────────────────────────────────────┘
```

#### Базовые примеры

**Извлечь первое поле (IP):**
```bash
awk '{print $1}' access.log
```

**Извлечь HTTP статус (поле #9 в Apache Combined Log):**
```bash
awk '{print $9}' access.log
```

**Извлечь несколько полей:**
```bash
awk '{print $1, $9}' access.log
# Вывод: IP и HTTP статус через пробел
# 185.220.101.47 404
```

**Извлечь последнее поле:**
```bash
awk '{print $NF}' access.log
```

**Вывести всю строку (эквивалент cat):**
```bash
awk '{print $0}' access.log
```

#### Кастомный разделитель полей

**По умолчанию:** пробел/tab

**Изменить разделитель:** флаг `-F`

**Пример — CSV файл (разделитель = запятая):**

```bash
# users.csv:
# john,doe,admin,john@example.com
# jane,smith,user,jane@example.com

# Извлечь email (4-е поле):
awk -F',' '{print $4}' users.csv
# Вывод:
# john@example.com
# jane@example.com
```

**Пример — /etc/passwd (разделитель = двоеточие):**

```bash
# Формат: username:x:uid:gid:comment:home:shell

# Извлечь username и home directory:
awk -F':' '{print $1, $6}' /etc/passwd
# Вывод:
# root /root
# www-data /var/www
# max /home/max
```

#### Условия в awk

**awk может фильтровать строки:**

```bash
# Только строки где $9 (HTTP status) = 404
awk '$9 == 404 {print $0}' access.log

# Только строки где $9 >= 400 (ошибки)
awk '$9 >= 400 {print $1, $9}' access.log

# Только строки содержащие "GET" в $6
awk '$6 ~ /GET/ {print $1, $6}' access.log
```

**Операторы:**
- `==` — равно
- `!=` — не равно
- `<`, `>`, `<=`, `>=` — сравнение чисел
- `~` — содержит pattern (regex)
- `!~` — НЕ содержит pattern

#### Встроенные переменные

| Переменная | Значение |
|------------|----------|
| `$0` | Вся строка |
| `$1, $2, $N` | Поля 1, 2, N |
| `$NF` | Последнее поле |
| `NF` | Количество полей в строке |
| `NR` | Номер текущей строки (Row Number) |

**Примеры:**

```bash
# Добавить номера строк:
awk '{print NR, $0}' file.txt

# Вывести строки с более чем 10 полями:
awk 'NF > 10 {print $0}' file.txt

# Вывести только строки 10-20:
awk 'NR >= 10 && NR <= 20 {print $0}' file.txt
```

#### awk = мини-программа

**awk — это язык программирования!** (полный Turing-complete язык)

**Базовая структура:**

```bash
awk 'BEGIN {действия ДО чтения файла}
     {действия для КАЖДОЙ строки}
     END {действия ПОСЛЕ чтения файла}' file.txt
```

**Пример — подсчёт суммы:**

```bash
# numbers.txt:
# 10
# 20
# 30

awk 'BEGIN {sum=0} {sum += $1} END {print "Total:", sum}' numbers.txt
# Вывод: Total: 60
```

**Но для нашей задачи достаточно:** `awk '{print $N}'` 🎯

> **LILITH:** *"awk — язык программирования. Можно писать скрипты на 1000 строк. Но 95% времени нужно только: `awk '{print $1}'`. Учи basics. Остальное — по необходимости. Не перегружай мозг."*

---

### 💻 Практика: Извлекаем поля (7 минут)

**Задание 1:** Извлеки IP адреса

```bash
cd artifacts/

# Только IP (первое поле):
awk '{print $1}' access.log | head -10
```

**Задание 2:** Извлеки HTTP статус коды

```bash
# Только статус (9-е поле):
awk '{print $9}' access.log | head -10

# Уникальные статусы:
awk '{print $9}' access.log | sort -u
```

**Задание 3:** IP + HTTP статус (два поля)

```bash
# IP и статус через пробел:
awk '{print $1, $9}' access.log | head -10

# Вывод:
# 185.220.101.47 404
# 91.234.56.78 401
# ...
```

**Задание 4:** Только ошибки 4xx и 5xx

```bash
# Фильтрация: статус >= 400
awk '$9 >= 400 {print $1, $9}' access.log | head -10

# Или через grep + awk:
grep -E " [45][0-9]{2} " access.log | awk '{print $1, $9}' | head -10
```

**Задание 5:** User-Agent (последнее поле в кавычках)

```bash
# User-Agent = последнее поле, но нужно через -F для кавычек
awk -F'"' '{print $6}' access.log | head -10

# Разбор:
# -F'"' = разделитель = кавычка
# Поля: [всё до 1-й кавычки] [внутри 1-х кавычек] ... [внутри 3-х кавычек = User-Agent]
```

**Задание 6:** Построй свой конвейер

**Цель:** TOP-5 User-Agents во время атаки

```bash
grep "03:47" access.log | awk -F'"' '{print $6}' | sort | uniq -c | sort -nr | head -5

# Вывод:
#    523 nmap NSE
#    234 sqlmap/1.5.2
#    156 curl/7.58.0
#    ...
```

---

### 🤔 Проверка понимания

**LILITH:** *"awk — ключевой инструмент. Проверка усвоения."*

**Вопрос 1:** В чём разница между `$0` и `$1` в awk?

<details>
<summary>Ответ</summary>

**Разница:**

- **`$0`** = вся строка целиком (до разбиения)
- **`$1`** = первое поле (после разбиения по пробелам)

**Пример:**

```bash
echo "hello world test" | awk '{print $0}'
# Вывод: hello world test

echo "hello world test" | awk '{print $1}'
# Вывод: hello

echo "hello world test" | awk '{print $2}'
# Вывод: world
```

**Когда использовать:**
- `$0` — когда нужна вся строка (редко, можно просто grep)
- `$1, $2, ...` — когда нужны конкретные поля (часто!)

**LILITH:** *"`$0` = весь пирог. `$1` = первый кусок. `$2` = второй кусок. awk режет пирог на куски по пробелам. Ты выбираешь какой кусок нужен."*

</details>

**Вопрос 2:** Почему `awk '{print $NF}'` выводит последнее поле?

<details>
<summary>Ответ</summary>

**`$NF` = последнее поле**

**NF (Number of Fields)** — встроенная переменная = количество полей в строке.

**Пример:**

```bash
echo "one two three four" | awk '{print NF}'
# Вывод: 4 (четыре поля)

echo "one two three four" | awk '{print $NF}'
# Вывод: four (поле номер 4 = последнее)
```

**Как работает:**
1. awk разбивает строку → 4 поля
2. `NF = 4`
3. `$NF` = `$4` = "four"

**Полезно когда:**
- Количество полей меняется
- Нужно последнее поле без подсчёта

**Пример — разные длины строк:**

```bash
echo -e "a b c\na b c d e\na b" | awk '{print $NF}'
# Вывод:
# c     (последнее из 3)
# e     (последнее из 5)
# b     (последнее из 2)
```

**LILITH:** *"`$NF` = универсальный способ взять хвост. Независимо от длины. Умный."*

</details>

**Вопрос 3:** Что делает `awk -F':' '{print $1}' /etc/passwd`?

<details>
<summary>Ответ</summary>

**Разбор команды:**

- **`-F':'`** = разделитель полей = двоеточие (не пробел!)
- **`'{print $1}'`** = вывести первое поле
- **`/etc/passwd`** = файл пользователей Unix

**Формат /etc/passwd:**
```
username:x:uid:gid:comment:home:shell
root:x:0:0:root:/root:/bin/bash
max:x:1000:1000:Maxim:/home/max:/bin/bash
```

**Результат команды:**
```bash
awk -F':' '{print $1}' /etc/passwd
# Вывод:
# root
# daemon
# bin
# max
# ...
```

**Выводит все usernames (первое поле до двоеточия).**

**Практическое применение:**
```bash
# Все пользователи:
awk -F':' '{print $1}' /etc/passwd

# Все home директории:
awk -F':' '{print $6}' /etc/passwd

# username + shell:
awk -F':' '{print $1, $7}' /etc/passwd
```

**LILITH:** *"Разные файлы = разные разделители. CSV = запятая. passwd = двоеточие. Логи = пробел. `-F` меняет правила игры. Адаптируйся."*

</details>

---

## Цикл 5: sort + uniq — статистика на лету (10-12 минут)

### 🎬 Сюжет: Подсчёт угроз

**13:00 — Лаборатория НГУ**

> **Ольга:**
> *"Ты знаешь grep, awk, pipes. Теперь финальный навык — подсчёт повторений. Кто атакует чаще всего?"*

Она печатает:

```bash
awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -10
```

**Вывод:**
```
    523 185.220.101.47
    234 45.155.205.67
    156 91.234.56.78
    134 203.0.113.45
     89 198.51.100.23
     67 192.0.2.10
     45 185.220.102.11
     34 91.234.57.22
     28 45.155.206.88
     23 203.0.114.99
```

> **Ольга:**
> *"TOP-10 атакующих. 523 запроса с одного IP — явная атака. Три команды: sort, uniq -c, sort -nr. Статистика."*

```
┌─────────────────────────────────────────────────────────────┐
│ LILITH:                                                       │
│                                                               │
│ "sort + uniq -c = счётчик частоты. Статистика без SQL.       │
│                                                               │
│  Представь зал с людьми. Задача: кто пришёл сколько раз?     │
│                                                               │
│  1. sort = построй людей в очередь по именам (John, John,    │
│     John, Mary, Mary, Peter)                                 │
│  2. uniq -c = посчитай соседей с одинаковыми именами         │
│     (3 John, 2 Mary, 1 Peter)                                │
│  3. sort -nr = пересортируй по количеству (от большего)      │
│     (John=3, Mary=2, Peter=1)                                │
│                                                               │
│  Результат: рейтинг популярности. TOP-N. Без баз данных."   │
└─────────────────────────────────────────────────────────────┘
```

---

### 📚 Теория: sort + uniq — подсчёт и статистика

#### sort — сортировка

**Базовый синтаксис:**
```bash
sort file.txt
```

**По умолчанию:**
- Сортирует строки в алфавитном порядке (лексикографически)
- По возрастанию (A → Z, 0 → 9)

**Важные флаги:**

| Флаг | Действие | Пример |
|------|----------|--------|
| `-r` | Reverse (обратный порядок) | `sort -r` |
| `-n` | Numeric sort (как числа, не строки) | `sort -n` |
| `-u` | Unique (только уникальные строки) | `sort -u` |
| `-k N` | Сортировать по N-му полю | `sort -k 2` |
| `-t 'X'` | Разделитель полей | `sort -t ',' -k 2` |

**Примеры:**

```bash
# Алфавитная сортировка:
echo -e "zebra\napple\nbanana" | sort
# Вывод: apple, banana, zebra

# Обратный порядок:
echo -e "zebra\napple\nbanana" | sort -r
# Вывод: zebra, banana, apple

# Числовая сортировка:
echo -e "100\n20\n3" | sort
# Вывод: 100, 20, 3 (лексикографически!)

echo -e "100\n20\n3" | sort -n
# Вывод: 3, 20, 100 (числовой порядок!)
```

**"Aha!" момент: числа vs строки:**

```bash
# ❌ Неправильно (строковая сортировка чисел):
echo -e "100\n20\n3" | sort
# Вывод:
# 100  ← "1" идёт до "2" в алфавите!
# 20
# 3

# ✅ Правильно (числовая сортировка):
echo -e "100\n20\n3" | sort -n
# Вывод:
# 3
# 20
# 100
```

> **LILITH:** *"`sort` видит строки, не числа. Для сортировки чисел: `-n` флаг. Забудешь `-n` — получишь 100 перед 20. Классическая ошибка."*

#### uniq — уникальные строки

**Базовый синтаксис:**
```bash
uniq file.txt
```

**Что делает:**
- Убирает **соседние** дубликаты (!)
- **Требует предварительную сортировку!**

**Важные флаги:**

| Флаг | Действие |
|------|----------|
| `-c` | Count (показать количество повторений) |
| `-d` | Duplicates (только дубликаты) |
| `-u` | Unique (только уникальные, без дубликатов) |

**Примеры:**

```bash
# Убрать соседние дубликаты:
echo -e "apple\napple\nbanana\napple" | uniq
# Вывод:
# apple
# banana
# apple  ← НЕ удалён! (не сосед первому)

# С сортировкой (правильно):
echo -e "apple\napple\nbanana\napple" | sort | uniq
# Вывод:
# apple  ← один раз
# banana

# Подсчёт повторений (-c):
echo -e "apple\napple\nbanana\napple" | sort | uniq -c
# Вывод:
#       3 apple
#       1 banana
```

#### Комбинация: sort | uniq -c | sort -nr

**Это классический pattern для TOP-N анализа!**

**Пошагово:**

```bash
# Шаг 1: Извлечь интересующие значения
awk '{print $1}' access.log

# Шаг 2: Сортировать (для uniq)
| sort

# Шаг 3: Подсчитать повторения
| uniq -c
# Вывод: "  количество значение"
#        "     523 185.220.101.47"

# Шаг 4: Сортировать по количеству (числовая, обратная)
| sort -nr
# -n = numeric
# -r = reverse (от большего к меньшему)

# Шаг 5: Взять TOP-10
| head -10
```

**Полная команда:**
```bash
awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -10
```

**Визуализация процесса:**

```
Исходные IP (неупорядоченные):
185.220.101.47
91.234.56.78
185.220.101.47
203.0.113.45
185.220.101.47
91.234.56.78
...

↓ sort

185.220.101.47
185.220.101.47
185.220.101.47
91.234.56.78
91.234.56.78
203.0.113.45
...

↓ uniq -c (подсчёт соседей)

    523 185.220.101.47
    156 91.234.56.78
    134 203.0.113.45
    ...

↓ sort -nr (сортировка по числу, reverse)

    523 185.220.101.47  ← больше всего!
    156 91.234.56.78
    134 203.0.113.45
    ...

↓ head -10

TOP-10 атакующих IP!
```

---

### 💻 Практика: Статистика логов (7 минут)

**Задание 1:** TOP-10 самых активных IP

```bash
cd artifacts/

awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -10
```

**Задание 2:** Статистика HTTP статус кодов

```bash
awk '{print $9}' access.log | sort | uniq -c | sort -nr

# Вывод:
#   3200 200
#    800 404
#    250 403
#    100 500
#     50 503
```

**Задание 3:** TOP-5 атакующих во время пика (03:47)

```bash
grep "03:47" access.log | awk '{print $1}' | sort | uniq -c | sort -nr | head -5
```

**Задание 4:** TOP-5 User-Agents

```bash
awk -F'"' '{print $6}' access.log | sort | uniq -c | sort -nr | head -5

# Вывод:
#    523 nmap NSE
#    234 sqlmap/1.5.2
#    156 curl/7.58.0
#     89 Nikto/2.1.6
#     67 ZmEu
```

**Задание 5:** Сколько уникальных IP атаковали?

```bash
awk '{print $1}' access.log | sort -u | wc -l

# Вывод: 87 (87 уникальных IP)
```

**Задание 6:** TOP-10 запрашиваемых путей (URLs)

```bash
awk '{print $7}' access.log | sort | uniq -c | sort -nr | head -10

# Вывод:
#    523 /admin
#    234 /wp-admin
#    156 /login
#    ...
```

---

### 🤔 Проверка понимания

**LILITH:** *"sort + uniq — хлеб и масло log analysis. Проверка."*

**Вопрос 1:** Почему `sort -n` важен при сортировке чисел?

<details>
<summary>Ответ</summary>

**Проблема: `sort` по умолчанию = лексикографическая сортировка (как строки)**

**Пример:**

```bash
echo -e "100\n20\n3\n1000" | sort
# Вывод:
# 1000  ← "1" < "2" < "3" в алфавите!
# 100
# 20
# 3
```

**С `-n` флагом:**

```bash
echo -e "100\n20\n3\n1000" | sort -n
# Вывод:
# 3
# 20
# 100
# 1000
```

**Когда критично:**
- Сортировка количеств (после `uniq -c`)
- Сортировка метрик, размеров файлов
- Любая числовая статистика

**Классическая ошибка:**
```bash
# ❌ Неправильно (без -n):
uniq -c file | sort -r | head -10
# Может вывести 9 перед 100!

# ✅ Правильно:
uniq -c file | sort -nr | head -10
```

**LILITH:** *"`sort` не умеет математику. Для него 9 > 1000 (лексикографически). Флаг `-n` = включить мозг."*

</details>

**Вопрос 2:** В чём разница между `sort -u` и `sort | uniq`?

<details>
<summary>Ответ</summary>

**Обе команды дают одинаковый результат:** уникальные отсортированные строки.

**Разница:**

1. **`sort -u`** — делает всё за один проход (быстрее, эффективнее)
   ```bash
   sort -u file.txt
   ```

2. **`sort | uniq`** — два отдельных процесса (больше накладных расходов)
   ```bash
   sort file.txt | uniq
   ```

**Когда использовать `sort | uniq`:**
- Нужен флаг uniq, например `-c` (подсчёт)
- Нужно только уникальные или только дубликаты (`-u` или `-d`)

**Когда использовать `sort -u`:**
- Просто нужны уникальные строки (без подсчёта)
- Лучшая производительность

**Примеры:**

```bash
# Просто уникальные строки:
sort -u file.txt        # ✅ Быстрее

# Подсчёт повторений:
sort file.txt | uniq -c  # ✅ Нужен uniq -c

# Только дубликаты:
sort file.txt | uniq -d  # ✅ Нужен uniq -d
```

**LILITH:** *"`sort -u` = оптимизация. Но `sort | uniq -c` = статистика. Выбирай по задаче."*

</details>

**Вопрос 3:** Как прочитать вывод `uniq -c`?

```bash
    523 185.220.101.47
    156 91.234.56.78
```

<details>
<summary>Ответ</summary>

**Формат:** `количество значение`

**Разбор:**
- **523** — количество повторений
- **185.220.101.47** — значение (IP адрес)

**Интерпретация:**
- IP `185.220.101.47` встретился 523 раза в логах
- IP `91.234.56.78` встретился 156 раз

**Практическое значение:**
- 523 запроса с одного IP = подозрительно!
- Нормальный пользователь: 1-10 запросов
- 500+ запросов = автоматизированная атака (скрипт, бот)

**Полный workflow:**

```bash
# Шаг 1: Извлечь IP
awk '{print $1}' access.log

# Шаг 2: Подсчитать
| sort | uniq -c

# Вывод:
#       1 192.0.2.1        ← легитимный пользователь
#       3 192.0.2.5        ← легитимный
#     523 185.220.101.47   ← АТАКА! 🚨
#     156 91.234.56.78     ← подозрительно

# Шаг 3: Отсортировать (TOP первыми)
| sort -nr
```

**LILITH:** *"Большие числа = red flags. 1-10 запросов = человек. 100+ = бот. 500+ = атака. uniq -c показывает кто агрессивный."*

</details>

---

## Цикл 6: sed — текстовый хирург (опционально, 8-10 минут)

### 🎬 Сюжет: Зачистка следов

**14:00 — Лаборатория НГУ**

> **Ольга:**
> *"Последний инструмент — sed. Stream editor. Замены, удаления, трансформации. На лету. Без открытия файла."*

> **Макс:**
> *"Для анализа логов нужен?"*

> **Ольга (качает головой):**
> *"Не обязательно. Но знать полезно. Пример: убрать лишние поля, анонимизировать IP, заменить patterns..."*

```
┌─────────────────────────────────────────────────────────────┐
│ LILITH:                                                       │
│                                                               │
│ "sed = stream editor. Текстовый хирург. Работает на лету.    │
│                                                               │
│  Представь конвейер с деталями. sed = робот который:         │
│  - Заменяет красные детали на синие (s/red/blue/)           │
│  - Удаляет бракованные (d)                                   │
│  - НЕ останавливает конвейер                                 │
│                                                               │
│  grep = фильтр (что оставить)                                │
│  sed = хирург (что изменить)                                 │
│                                                               │
│  Для анализа логов? 80% времени не нужен. Но когда нужен —  │
│  незаменим."                                                 │
└─────────────────────────────────────────────────────────────┘
```

---

### 📚 Теория: sed — замены и трансформации

#### Базовый синтаксис

```bash
sed 's/pattern/replacement/' file.txt
```

**Что делает:**
- `s` = substitute (заменить)
- Заменяет первое вхождение `pattern` на `replacement` в каждой строке
- Выводит результат в stdout (файл не меняется!)

#### Базовые операции

**1. Замена (substitute):**

```bash
# Заменить "error" на "ERROR" (первое вхождение в строке):
sed 's/error/ERROR/' file.txt

# Заменить ВСЕ вхождения в строке (флаг g = global):
sed 's/error/ERROR/g' file.txt

# Case-insensitive замена (флаг i):
sed 's/error/ERROR/gi' file.txt
```

**2. Удаление строк:**

```bash
# Удалить строки содержащие "test":
sed '/test/d' file.txt

# Удалить пустые строки:
sed '/^$/d' file.txt

# Удалить строки 1-10:
sed '1,10d' file.txt
```

**3. Замена в файле (in-place editing):**

```bash
# -i = изменить файл (осторожно!):
sed -i 's/old/new/g' file.txt

# -i.bak = создать backup перед изменением:
sed -i.bak 's/old/new/g' file.txt
```

#### Практические примеры для логов

**Анонимизировать последний октет IP:**

```bash
# 185.220.101.47 → 185.220.101.XXX
sed 's/\([0-9]\{1,3\}\.\)\{3\}[0-9]\{1,3\}/\1XXX/g' access.log
```

**(Сложно! Для этого обычно используют специализированные tools)**

**Убрать timestamp (для упрощения):**

```bash
# Удалить всё в квадратных скобках:
sed 's/\[.*\]//g' access.log
```

**Заменить GET на [GET]:**

```bash
sed 's/GET/[GET]/g' access.log
```

#### Когда sed нужен для log analysis?

**Честно: редко!** grep + awk + pipes покрывают 90% задач.

**sed полезен для:**
- Анонимизации данных
- Cleanup перед анализом
- Трансформации форматов (CSV → TSV)
- Массовые замены в конфигах

**Для простого анализа логов:** `sed` **опционален**. ✅

> **LILITH:** *"sed — мощный. Но grep + awk достаточно для 90% задач. Учи sed когда понадобится. Сейчас фокус на grep/awk/pipes. Не перегружайся."*

---

### 💻 Практика: sed basics (5 минут, опционально)

**Задание 1:** Заменить статус 200 на OK

```bash
cd artifacts/

sed 's/ 200 / OK /g' access.log | head -5
```

**Задание 2:** Убрать timestamp

```bash
sed 's/\[.*\]//g' access.log | head -5
```

**Задание 3:** Удалить строки со статусом 200 (оставить только ошибки)

```bash
sed '/ 200 /d' access.log | head -10
```

**Задание 4:** Комбинация sed + awk

```bash
# Убрать timestamp, потом извлечь IP:
sed 's/\[.*\]//g' access.log | awk '{print $1}' | sort -u | head -10
```

---

### 🤔 Проверка понимания

**LILITH:** *"sed — опциональный для этой задачи. Быстрая проверка."*

**Вопрос:** В чём разница между `grep -v` и `sed '/pattern/d'`?

<details>
<summary>Ответ</summary>

**Обе команды удаляют строки содержащие pattern.**

**Синтаксис:**

```bash
# grep -v (invert match):
grep -v "200" access.log

# sed delete:
sed '/200/d' access.log
```

**Результат:** одинаковый (строки без "200")

**Когда использовать:**

- **`grep -v`** — проще, быстрее, читабельнее ✅
- **`sed '/pattern/d'`** — если уже используешь sed для других операций

**Производительность:** `grep` обычно быстрее (оптимизирован для поиска)

**Рекомендация:**
```bash
# ✅ Предпочтительно:
grep -v "200" access.log

# ✅ Если уже используешь sed:
sed '/200/d; s/old/new/g' access.log
```

**LILITH:** *"`grep -v` проще и быстрее. Используй sed когда нужны замены + удаления вместе. Don't overcomplicate."*

</details>

---

## Цикл 7: Финальная автоматизация — Type B подход (12-15 минут)

### 🎬 Сюжет: Отчёт для Anna

**15:00 — Лаборатория НГУ**

Звонок Анны:

> **Анна:**
> *"Максим, времени мало. Мне нужен отчёт. TOP-10 атакующих, HTTP статусы, проверка threat database. Можешь?"*

> **Ольга (тихо Максу):**
> *"Ты знаешь все инструменты. Построй one-liners. Оберни в лёгкий скрипт для автоматизации."*

```
┌─────────────────────────────────────────────────────────────┐
│ LILITH:                                                       │
│                                                               │
│ "Type B episode. Фокус на ИНСТРУМЕНТАХ, не на bash.          │
│                                                               │
│  Ты построишь:                                               │
│  - ONE-LINER для TOP-10 IP                                   │
│  - ONE-LINER для HTTP статусов                               │
│  - ONE-LINER для проверки threats                            │
│  - ONE-LINER для User-Agents                                 │
│                                                               │
│  Минимальный bash wrapper (30-40 строк) = клей между         │
│  командами.                                                  │
│                                                               │
│  70% Linux tools / 30% bash glue. Это правильный баланс."    │
└─────────────────────────────────────────────────────────────┘
```

---

### 📚 Финальное задание: log_analyzer.sh (Type B!)

#### Требования

Anna нужен скрипт который:

1. ✅ Анализирует `access.log`
2. ✅ Выводит TOP-10 атакующих IP
3. ✅ Выводит статистику HTTP статус кодов
4. ✅ Проверяет IP из `suspicious_ips.txt` (threat database)
5. ✅ Выводит TOP-5 User-Agents
6. ✅ Сохраняет результаты в `final_report.txt`

#### Type B подход: инструменты first!

**НЕ пиши большой bash скрипт с циклами!**
**Используй one-liners на базе grep/awk/sort/uniq!**

**Структура скрипта:**

```bash
#!/bin/bash
# Минимальный wrapper для склейки one-liners

LOG="$1"
THREATS="suspicious_ips.txt"
REPORT="final_report.txt"

echo "=== ANALYSIS REPORT ===" > "$REPORT"
echo "" >> "$REPORT"

# ONE-LINER 1: TOP-10 атакующих
echo "TOP-10 Attacking IPs:" >> "$REPORT"
grep "03:47" "$LOG" | awk '{print $1}' | sort | uniq -c | sort -nr | head -10 >> "$REPORT"

# ONE-LINER 2: HTTP статусы
echo "" >> "$REPORT"
echo "HTTP Status Codes:" >> "$REPORT"
awk '{print $9}' "$LOG" | sort | uniq -c | sort -nr >> "$REPORT"

# ONE-LINER 3: Проверка threats
echo "" >> "$REPORT"
echo "Threat Database Check:" >> "$REPORT"
while read ip; do
  count=$(grep -c "$ip" "$LOG")
  [[ $count -gt 0 ]] && echo "  FOUND: $ip ($count requests)" >> "$REPORT"
done < "$THREATS"

# ONE-LINER 4: TOP-5 User-Agents
echo "" >> "$REPORT"
echo "TOP-5 User-Agents:" >> "$REPORT"
grep "03:47" "$LOG" | awk -F'"' '{print $6}' | sort | uniq -c | sort -nr | head -5 >> "$REPORT"

echo "" >> "$REPORT"
echo "Report saved: $REPORT"
```

**Это Type B!**
- ✅ 80% работы = инструменты (grep, awk, sort, uniq)
- ✅ 20% работы = bash (переменные, while для threats)
- ✅ Фокус на использовании tools, не на bash логике

---

### 💻 Практика: Построй свой log_analyzer.sh (10 минут)

**Шаги:**

1. **Открой starter.sh:**
   ```bash
   cd ~/kernel-shadows/season-1-shell-foundations/episode-03-text-processing
   cat starter.sh
   ```

2. **Заполни TODO секции:**
   - ONE-LINER для TOP-10 IP
   - ONE-LINER для HTTP статусов
   - Loop для проверки threats
   - ONE-LINER для User-Agents

3. **Запусти:**
   ```bash
   chmod +x starter.sh
   ./starter.sh artifacts/access.log
   ```

4. **Проверь результат:**
   ```bash
   cat final_report.txt
   ```

5. **Запусти тесты:**
   ```bash
   ./tests/test.sh
   ```

---

### 🎯 Ключевые ONE-LINERS для финала

**1. TOP-10 атакующих IP (во время атаки 03:47):**
```bash
grep "03:47" access.log | awk '{print $1}' | sort | uniq -c | sort -nr | head -10
```

**2. HTTP статус коды (статистика):**
```bash
awk '{print $9}' access.log | sort | uniq -c | sort -nr
```

**3. Проверка threat database:**
```bash
while read ip; do
  count=$(grep -c "$ip" access.log)
  [[ $count -gt 0 ]] && echo "FOUND: $ip ($count requests)"
done < suspicious_ips.txt
```

**4. TOP-5 User-Agents (инструменты атаки):**
```bash
grep "03:47" access.log | awk -F'"' '{print $6}' | sort | uniq -c | sort -nr | head -5
```

**5. Уникальные IP (общее количество):**
```bash
awk '{print $1}' access.log | sort -u | wc -l
```

**Это инструменты, не bash программирование!** ✅

---

### 🤔 Финальная проверка

**LILITH:** *"Финальная проверка. Type B episode. Готов?"*

**Вопрос:** Почему этот episode Type B, не Type A?

<details>
<summary>Ответ</summary>

**Type A (Bash Automation):**
- 60% bash scripting (циклы, условия, функции)
- 40% Linux commands
- Фокус на **программировании** в bash

**Type B (Linux Tools):**
- 70% Linux tools (grep, awk, sort, uniq, pipes)
- 30% bash (минимальный wrapper)
- Фокус на **использовании инструментов**

**Episode 03 = Type B потому что:**

✅ **Основное время** = изучение grep, awk, pipes, sort, uniq
✅ **Практика** = построение one-liners
✅ **Финальный скрипт** = склейка one-liners (30-40 строк)
✅ **Цель** = научить использовать tools, не писать bash

**Если бы это был Type A:**
- 200+ строк bash
- Ручной парсинг логов через циклы
- Самописный счётчик вместо `uniq -c`
- Переизобретение велосипеда

**Type B философия:**
> *"Используй готовые инструменты. Bash — клей между ними, не замена."*

**LILITH:** *"grep/awk/sed существуют 50 лет. Оптимизированы. Протестированы миллионами пользователей. Переписывать их в bash = глупость. Используй. Автоматизируй минимальным wrapper. Это Type B."*

</details>

---

## 🎬 Эпилог: Результаты

**16:00 — Лаборатория НГУ**

Макс запускает финальный скрипт:

```bash
./log_analyzer.sh artifacts/access.log
```

**Вывод в терминале:**
```
Analyzing artifacts/access.log...

TOP-10 Attacking IPs:
    523 185.220.101.47
    234 45.155.205.67
    156 91.234.56.78
    ...

HTTP Status Codes:
   3200 200
    800 404
    250 403
    ...

Threat Database Check:
  FOUND: 185.220.101.47 (523 requests) ⚠️
  FOUND: 45.155.205.67 (234 requests) ⚠️

TOP-5 User-Agents:
    523 nmap NSE
    234 sqlmap/1.5.2
    156 curl/7.58.0
    ...

Report saved: final_report.txt
```

Макс отправляет `final_report.txt` Анне.

**17:00 — Видеозвонок с Анной**

> **Анна (изучает отчёт):**
> *"Хорошая работа. 185.220.101.47 — известный Tor exit node. 523 запроса за минуту. Атака подтверждена."*

> **Анна:**
> *"nmap, sqlmap, curl — инструменты разведки. Они искали уязвимости. Не нашли. Но это начало."*

Пауза.

> **Анна:**
> *"Крылов усиливает давление. Мы усиливаем защиту. Продолжаем."*

Звонок обрывается.

> **Ольга (Максу):**
> *"Ты за 6 часов освоил text processing. grep, awk, pipes, sort, uniq. Теперь ты можешь анализировать терабайты логов. Инструменты в руках."*

```
┌─────────────────────────────────────────────────────────────┐
│ LILITH:                                                       │
│                                                               │
│ "Episode 03 завершён.                                        │
│                                                               │
│  Ты выучил:                                                  │
│  ✅ grep — поиск patterns                                    │
│  ✅ awk — извлечение колонок                                 │
│  ✅ pipes — конвейер данных                                  │
│  ✅ sort + uniq — статистика                                 │
│  ✅ ONE-LINERS — команды вместо скриптов                     │
│                                                               │
│  Это Type B episode. 70% инструменты, 30% bash.              │
│  Так работают настоящие Linux администраторы.                │
│                                                               │
│  Следующий episode — Package Management. Установка           │
│  инструментов, конфигурирование систем. Продолжаем."         │
└─────────────────────────────────────────────────────────────┘
```

---

## 📊 Итоги Episode 03

### Что ты освоил:

#### Инструменты (70%):
- ✅ **grep** — поиск patterns, regex, фильтрация строк
- ✅ **awk** — извлечение колонок, обработка полей
- ✅ **pipes** — цепочки команд, конвейер обработки
- ✅ **sort** — сортировка (алфавитная, числовая)
- ✅ **uniq** — уникальные значения, подсчёт повторений
- ✅ **sed** — замены текста (опционально)

#### Bash (30%):
- ✅ Минимальный wrapper для склейки команд
- ✅ Переменные, перенаправление вывода
- ✅ while read для обработки файлов

#### Концепции:
- ✅ Unix философия: "Do One Thing Well"
- ✅ Type B episode: инструменты first, bash second
- ✅ ONE-LINERS вместо больших скриптов
- ✅ Математика эффективности (3.2 года vs 10 секунд)

---

## 📈 Статистика

**Время прохождения:** 2-2.5 часа
**Циклов:** 7 (по 10-15 минут)
**Метафор:** 6 (Океан+сонар, Детектив, Конвейер, Швейцарский нож, Счётчик, Хирург)
**ASCII диаграмм:** 5
**"Aha!" моментов:** 5
**LILITH цитат:** 18
**Упражнений:** 7

**Баланс контента:**
- Linux Tools: **70%** ✅
- Bash Automation: **30%** ✅
- Type B Episode: ✅

---

## 🎯 Следующие шаги

1. **Завершить задание:**
   - Построить `log_analyzer.sh` через one-liners
   - Запустить тесты: `./tests/test.sh`
   - Проверить `final_report.txt`

2. **Практика (опционально):**
   - Анализируй логи на production серверах
   - Строй свои one-liners для задач
   - Изучи regex глубже (когда понадобится)

3. **Episode 04: Package Management**
   - apt, dpkg, systemd
   - Установка инструментов
   - Конфигурирование сервисов

---

## 💡 Шпаргалка: Основные one-liners

```bash
# TOP-N самых частых значений:
COMMAND | sort | uniq -c | sort -nr | head -N

# Уникальные значения (количество):
COMMAND | sort -u | wc -l

# Извлечь N-е поле:
awk '{print $N}' file

# Фильтр + извлечение:
grep "pattern" file | awk '{print $1}'

# Проверка списка:
while read item; do grep "$item" file; done < list.txt

# Статистика:
awk '{print $FIELD}' file | sort | uniq -c | sort -nr
```

---

**КОНЕЦ EPISODE 03** ✅

---

