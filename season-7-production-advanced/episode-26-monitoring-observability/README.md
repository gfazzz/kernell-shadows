# EPISODE 26: MONITORING & OBSERVABILITY
**Season 7: Production & Advanced Topics** | –†–µ–π–∫—å—è–≤–∏–∫, –ò—Å–ª–∞–Ω–¥–∏—è üáÆüá∏

> *"Without monitoring ‚Äî you are blind. With bad monitoring ‚Äî you are blind with false confidence. Good monitoring shows truth: what works, what breaks, what matters."* ‚Äî Gu√∞r√∫n √Åsta

---

## üìã –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û–ë –≠–ü–ò–ó–û–î–ï

- **–õ–æ–∫–∞—Ü–∏—è:** –†–µ–π–∫—å—è–≤–∏–∫, Verne Global Datacenter (monitoring center)
- **–î–Ω–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏:** 51-52 –∏–∑ 60
- **–í—Ä–µ–º—è –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è:** 5-6 —á–∞—Å–æ–≤
- **–°–ª–æ–∂–Ω–æ—Å—Ç—å:** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ
- **–¢–∏–ø:** Type B (Configuration ‚Äî Prometheus + Grafana, minimal bash)

**–ü–µ—Ä—Å–æ–Ω–∞–∂–∏:**
- **–ú–∞–∫—Å –°–æ–∫–æ–ª–æ–≤** ‚Äî –≥–ª–∞–≤–Ω—ã–π –≥–µ—Ä–æ–π (–≤—ã), 49 –¥–Ω–µ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ–∑–∞–¥–∏
- **–í–∏–∫—Ç–æ—Ä –ü–µ—Ç—Ä–æ–≤** ‚Äî –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –æ–ø–µ—Ä–∞—Ü–∏–∏
- **Gu√∞r√∫n √Åsta** ‚Äî Monitoring engineer, ex-DataDog, observability expert
- **Bj√∂rn Sigurdsson** ‚Äî Kubernetes SRE (–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç)
- **LILITH v7.0** ‚Äî AI-–ø–æ–º–æ—â–Ω–∏–∫ (Production Mode)

**–¶–µ–ª—å —ç–ø–∏–∑–æ–¥–∞:** –ù–∞—Å—Ç—Ä–æ–∏—Ç—å production monitoring –¥–ª—è Kubernetes infrastructure (Prometheus + Grafana + Alertmanager)

---

## üé¨ PROLOGUE: –ë–ï–ó –ì–õ–ê–ó ‚Äî –°–õ–ï–ü–û–ô

### –î–µ–Ω—å 51, 09:00 ‚Äî Verne Global Monitoring Center

–ú–∞–∫—Å –≤—Ö–æ–¥–∏—Ç –≤ monitoring center. –°—Ç–µ–Ω–∞ –∏–∑ –º–æ–Ω–∏—Ç–æ—Ä–æ–≤: –≥—Ä–∞—Ñ–∏–∫–∏ CPU, Memory, Network, Kubernetes pods. Real-time dashboards –º–∏–≥–∞—é—Ç –∑–µ–ª—ë–Ω—ã–º –∏ –∂—ë–ª—Ç—ã–º. –í —Ü–µ–Ω—Ç—Ä–µ –∫–æ–º–Ω–∞—Ç—ã —Å—Ç–æ–∏—Ç –∂–µ–Ω—â–∏–Ω–∞ —Å —Ä—ã–∂–∏–º–∏ –≤–æ–ª–æ—Å–∞–º–∏, —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∏.

**Gu√∞r√∫n:** "Welcome. I'm Gu√∞r√∫n √Åsta. Monitoring engineer. Ex-DataDog, now here. Bj√∂rn told me about you ‚Äî Kubernetes deployed, pods running, services exposed. Good. But question: how you know it works?"

**–ú–∞–∫—Å:** "–Ø –º–æ–≥—É –ø—Ä–æ–≤–µ—Ä–∏—Ç—å ‚Äî kubectl get pods, –≤—Å—ë Running."

**Gu√∞r√∫n:** "Manual check. Every 5 minutes you run command? What if pod crashes at night? What if CPU 90% but still Running? What if memory leak grows slowly for 3 days?"

*[Gu√∞r√∫n –∫–ª–∏–∫–∞–µ—Ç –º—ã—à–∫–æ–π ‚Äî –Ω–∞ —ç–∫—Ä–∞–Ω–µ –ø–æ—è–≤–ª—è–µ—Ç—Å—è Grafana dashboard —Å 20 –≥—Ä–∞—Ñ–∏–∫–∞–º–∏]*

**Gu√∞r√∫n:** "This is production monitoring. Not 'kubectl get pods' every hour. Real-time metrics. Every second. CPU, memory, disk, network, HTTP requests, database queries, cache hit rate. Everything. And when something wrong ‚Äî alert fires. SMS, email, Slack. Before users notice problem."

**–ú–∞–∫—Å:** "–í–ø–µ—á–∞—Ç–ª—è—é—â–µ. –°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ —ç—Ç–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å?"

**Gu√∞r√∫n:** "For simple setup ‚Äî 2 hours. For production-ready ‚Äî 2 days. Today you learn basics: Prometheus collects metrics. Grafana visualizes. Alertmanager sends alerts. Tomorrow you have eyes."

**LILITH:** "–ú–∞–∫—Å, Episode 25 –¥–∞–ª —Ç–µ–±–µ Kubernetes. Episode 26 ‚Äî –≥–ª–∞–∑–∞ –¥–ª—è Kubernetes. Without monitoring ‚Äî —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å –∫–∞–∫ –¥–æ–ª–≥–æ. With monitoring ‚Äî —Ç—ã –≤–∏–¥–∏—à—å –≤—Å—ë: health, performance, trends, anomalies. Production —Ç—Ä–µ–±—É–µ—Ç visibility."

---

## üîÑ –¶–ò–ö–õ 1: WHY MONITORING? (10 –º–∏–Ω—É—Ç)

### üé¨ –°—é–∂–µ—Ç (2 –º–∏–Ω)

**Gu√∞r√∫n:** "Before we start ‚Äî I tell story. DataDog, 2019. Customer runs e-commerce. Black Friday. 10x traffic. Everything looks good ‚Äî 'kubectl get pods' shows all Running. Then ‚Äî users report: checkout –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. We investigate. Problem: memory leak in payment service. Pod Running, but service dead inside. Took 2 hours to find. Lost $500,000 revenue."

**Gu√∞r√∫n:** "If monitoring existed? Alert fires: 'Payment service response time > 5 seconds.' We investigate immediately. Fix in 10 minutes. Save $500,000. This is why monitoring matters."

**–ú–∞–∫—Å:** "–ó–Ω–∞—á–∏—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ‚Äî —ç—Ç–æ not just '—Å–º–æ—Ç—Ä–µ—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏', –∞ '–∑–Ω–∞—Ç—å –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Ä–∞–Ω—å—à–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π'?"

**Gu√∞r√∫n:** "Exactly. Monitoring is early warning system. Problems always happen. Question is: who notices first ‚Äî you or users? If you ‚Äî fix before impact. If users ‚Äî reputation damage, money loss, trust broken."

### üìö –¢–µ–æ—Ä–∏—è: Observability Triangle (5-7 –º–∏–Ω)

**–ú–µ—Ç–∞—Ñ–æ—Ä–∞: Monitoring = –î–∞—Ç—á–∏–∫–∏ –≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ**

–ü—Ä–µ–¥—Å—Ç–∞–≤—å –º–∞—à–∏–Ω—É –±–µ–∑ dashboard:
- **–°–∫–æ—Ä–æ—Å—Ç—å?** –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ (—Å–º–æ—Ç—Ä–∏—à—å –Ω–∞ –ø–µ–π–∑–∞–∂ –∑–∞ –æ–∫–Ω–æ–º)
- **–¢–æ–ø–ª–∏–≤–æ?** –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ (–ø–æ–∫–∞ –Ω–µ –∑–∞–≥–ª–æ—Ö–ª–∞)
- **–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–≤–∏–≥–∞—Ç–µ–ª—è?** –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ (–ø–æ–∫–∞ –¥—ã–º –Ω–µ –ø–æ—à—ë–ª)

–ú–∞—à–∏–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ —Ç—ã —Å–ª–µ–ø–æ–π. –û–¥–∏–Ω –¥–∞—Ç—á–∏–∫ —Å–ª–æ–º–∞–ª—Å—è ‚Äî —Ç—ã –Ω–µ —É–∑–Ω–∞–µ—à—å –ø–æ–∫–∞ –Ω–µ –ø–æ–∑–¥–Ω–æ.

**Production —Å–∏—Å—Ç–µ–º–∞ = —Ç–∞ –∂–µ –º–∞—à–∏–Ω–∞.** –ë–µ–∑ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:
- CPU 100%? –£–∑–Ω–∞–µ—à—å –∫–æ–≥–¥–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–∏—Å–Ω–µ—Ç
- Memory leak? –£–∑–Ω–∞–µ—à—å –∫–æ–≥–¥–∞ OOMKilled
- Disk 95% full? –£–∑–Ω–∞–µ—à—å –∫–æ–≥–¥–∞ –∑–∞–ø–∏—Å—å fails

**Observability Triangle** (3 —Å—Ç–æ–ª–ø–∞ monitoring):

```
         üìä METRICS
        (Prometheus)
       /            \
      /              \
     /                \
    /                  \
   /                    \
üìù LOGS          üîç TRACES
(Loki/ELK)      (Jaeger)
```

**1. Metrics** (—á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ):
- CPU usage: 45%
- Memory usage: 2.3 GB
- HTTP requests/sec: 1,240
- Response time: 150ms
- Error rate: 0.02%

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** Trends, alerting, dashboards (Episode 26 —Ñ–æ–∫—É—Å)

**2. Logs** (—Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è):
- `2025-10-13 12:34:56 ERROR Database connection timeout`
- `2025-10-13 12:35:10 WARN Retry attempt 3/5`
- `2025-10-13 12:35:15 INFO Request processed successfully`

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** Debugging, audit trails (—É–∂–µ –∏–∑—É—á–µ–Ω–æ –≤ Season 1-3)

**3. Traces** (request flow):
- Request path: API ‚Üí Auth Service ‚Üí Database ‚Üí Cache ‚Üí Response
- Each step timing: 10ms ‚Üí 50ms ‚Üí 200ms ‚Üí 5ms ‚Üí 15ms
- Bottleneck: Database (200ms)

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** Performance debugging, microservices (advanced, beyond scope)

**Episode 26 —Ñ–æ–∫—É—Å: Metrics** (Prometheus + Grafana)

---

### **Prometheus: Time-series database –¥–ª—è metrics**

**–ß—Ç–æ —ç—Ç–æ:**
- Open-source monitoring system (CNCF project, –∫–∞–∫ Kubernetes)
- –°–æ–±–∏—Ä–∞–µ—Ç metrics —á–µ—Ä–µ–∑ HTTP endpoints (scraping)
- –•—Ä–∞–Ω–∏—Ç –∫–∞–∫ time-series data (timestamp + value)
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç powerful query language (PromQL)

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   PROMETHEUS SERVER                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ Retrieval‚îÇ‚îÄ‚îÄ‚Üí‚îÇTime-series‚îÇ‚îÄ‚îÄ‚Üí‚îÇ  HTTP Server ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ (scrape) ‚îÇ   ‚îÇ  Database ‚îÇ   ‚îÇ   (API)      ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ       ‚Üì                                  ‚Üë               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ  ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Alertmanager ‚îÇ (alerts)                    ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì                           ‚Üë
    (scrape metrics)            (query metrics)
         ‚Üì                           ‚Üë
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TARGETS        ‚îÇ          ‚îÇ   GRAFANA    ‚îÇ
‚îÇ  - Kubernetes   ‚îÇ          ‚îÇ  (dashboards)‚îÇ
‚îÇ  - Node Exporter‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  - Apps         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**

1. **Targets expose metrics:**
```
# Kubernetes pod exposes /metrics endpoint
http://my-app:8080/metrics

# Output (Prometheus format):
http_requests_total{method="GET",status="200"} 1523
http_requests_total{method="POST",status="201"} 342
memory_usage_bytes 2147483648
cpu_usage_seconds_total 456.78
```

2. **Prometheus scrapes targets** (pulls metrics every 15-30 seconds):
```yaml
scrape_configs:
  - job_name: 'kubernetes-pods'
    scrape_interval: 15s
    kubernetes_sd_configs:
      - role: pod
```

3. **Data stored as time-series:**
```
Metric: http_requests_total{method="GET"}
Timestamp 1: 1000 requests
Timestamp 2: 1050 requests (+50 in 15 sec)
Timestamp 3: 1120 requests (+70 in 15 sec)
...
```

4. **Query with PromQL:**
```promql
rate(http_requests_total[5m])  # Requests per second (5-min window)
```

5. **Alerting rules:**
```yaml
- alert: HighCPU
  expr: cpu_usage > 0.9
  for: 5m
  annotations:
    summary: "CPU usage > 90% for 5 minutes"
```

---

### **Grafana: Visualization –¥–ª—è Prometheus**

**–ß—Ç–æ —ç—Ç–æ:**
- Open-source dashboards and visualization
- Connects to Prometheus (and other data sources)
- Beautiful graphs, charts, tables
- Templating, variables, annotations

**–ü–æ—á–µ–º—É –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç Prometheus?**
- Prometheus = data collection & storage (backend)
- Grafana = visualization & exploration (frontend)
- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ concerns: Prometheus —Ñ–æ–∫—É—Å –Ω–∞ reliability, Grafana –Ω–∞ UX

---

### **Alertmanager: Notifications –¥–ª—è –ø—Ä–æ–±–ª–µ–º**

**–ß—Ç–æ —ç—Ç–æ:**
- Handles alerts –æ—Ç Prometheus
- Deduplication (same alert –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è 100 —Ä–∞–∑)
- Grouping (multiple related alerts ‚Üí one notification)
- Routing (—Ä–∞–∑–Ω—ã–µ alerts ‚Üí —Ä–∞–∑–Ω—ã–µ –∫–∞–Ω–∞–ª—ã)
- Silencing (–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å alerts)

**Alert flow:**
```
Prometheus detects: CPU > 90%
      ‚Üì
Prometheus fires alert ‚Üí Alertmanager
      ‚Üì
Alertmanager processes:
  - Deduplicates (same alert already sent?)
  - Groups (CPU alert + Memory alert = "Server Issues")
  - Routes to channel (Slack/Email/PagerDuty)
      ‚Üì
Notification sent ‚Üí Team responds
```

---

**Best Practices:**

‚úÖ **DO:**
- Monitor —á—Ç–æ –≤–∞–∂–Ω–æ (not everything)
- Set alerts –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã (not cosmetic issues)
- Use dashboards –¥–ª—è exploration, alerts –¥–ª—è action
- Define SLOs (Service Level Objectives) ‚Äî target metrics

‚ùå **DON'T:**
- Alert –Ω–∞ –≤—Å—ë (alert fatigue ‚Üí ignored alerts)
- Monitor metrics you don't understand
- Set alerts –±–µ–∑ runbooks (—á—Ç–æ –¥–µ–ª–∞—Ç—å –∫–æ–≥–¥–∞ alert fires?)
- –ó–∞–±—ã–≤–∞–π –ø—Ä–æ retention (metrics —Ö—Ä–∞–Ω—è—Ç—Å—è 15 –¥–Ω–µ–π default, –Ω–∞—Å—Ç—Ä–æ–π –¥–ª—è production)

**Gu√∞r√∫n's wisdom:**
> "Good monitoring answers 3 questions: What is broken? Why is broken? What to do? Bad monitoring shows 1000 metrics but no answers. Focus on actionable insights, not vanity metrics."

**LILITH:**
> "Monitoring ‚Äî —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ '—Å–º–æ—Ç—Ä–µ—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Ä–∞–¥–æ–≤–∞—Ç—å—Å—è –∑–µ–ª—ë–Ω—ã–º –ª–∏–Ω–∏—è–º'. –≠—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–Ω–Ω–µ–≥–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è. CPU 80% ‚Äî –µ—â—ë –Ω–µ –ø—Ä–æ–±–ª–µ–º–∞. CPU 80% –∏ —Ä–∞—Å—Ç—ë—Ç ‚Äî –ø—Ä–æ–±–ª–µ–º–∞ —á–µ—Ä–µ–∑ 10 –º–∏–Ω—É—Ç. Monitoring –≤–∏–¥–∏—Ç —Ç—Ä–µ–Ω–¥—ã, –Ω–µ —Ç–æ–ª—å–∫–æ snapshots. Kubectl –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç '—Å–µ–π—á–∞—Å'. Prometheus –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç '—á—Ç–æ –±—ã–ª–æ, —á—Ç–æ –±—É–¥–µ—Ç'."

### üíª –ü—Ä–∞–∫—Ç–∏–∫–∞ (3-5 –º–∏–Ω)

**–ó–∞–¥–∞–Ω–∏–µ 1: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞**

Scenario: Production web app, 1000 req/sec.

–ö–∞–∫–∏–µ metrics –≤–∞–∂–Ω—ã –¥–ª—è monitoring? (–≤—ã–±–µ—Ä–∏ 5 —Å–∞–º—ã—Ö –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö)

<details>
<summary>üí° –î—É–º–∞–π –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π</summary>

**Top 5 –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫:**

1. **Request rate** (requests/sec)
   - –ü–æ—á–µ–º—É: –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç traffic spikes –∏–ª–∏ drops
   - Alert: –ï—Å–ª–∏ —É–ø–∞–ª –Ω–∞ 50% ‚Üí –≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞

2. **Error rate** (%  HTTP 5xx errors)
   - –ü–æ—á–µ–º—É: –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç broken functionality
   - Alert: –ï—Å–ª–∏ > 1% ‚Üí users affected

3. **Response time** (p95 latency)
   - –ü–æ—á–µ–º—É: User experience metric
   - Alert: –ï—Å–ª–∏ > 500ms ‚Üí slow –¥–ª—è users

4. **CPU usage** (%)
   - –ü–æ—á–µ–º—É: Resource saturation indicator
   - Alert: –ï—Å–ª–∏ > 90% ‚Üí risk of crash

5. **Memory usage** (%)
   - –ü–æ—á–µ–º—É: Memory leaks detection
   - Alert: –ï—Å–ª–∏ —Ä–∞—Å—Ç—ë—Ç continuously ‚Üí OOMKilled soon

**–¢–∞–∫–∂–µ –ø–æ–ª–µ–∑–Ω—ã (–Ω–æ –Ω–µ top 5):**
- Disk I/O, Network bandwidth, Database connections, Cache hit rate

**Vanity metrics (–Ω–µ critical):**
- Total users count (–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –Ω–æ –Ω–µ –¥–ª—è alerts)
- Number of features (–Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ availability)

</details>

### ü§î –í–æ–ø—Ä–æ—Å LILITH (1 –º–∏–Ω)

**LILITH:** "Kubernetes pod status: Running. CPU: 95%. Memory: 90%. Response time: 10 seconds (–±—ã–ª–æ 100ms). Kubectl –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç 'Running'. –ü—Ä–æ–±–ª–µ–º–∞ –µ—Å—Ç—å?"

<details>
<summary>üí° –î—É–º–∞–π –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π</summary>

**–û—Ç–≤–µ—Ç:** **–î–ê, —Å–µ—Ä—å—ë–∑–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞!**

**–ü–æ—á–µ–º—É:**
- Pod status = Running (–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∂–∏–≤)
- –ù–æ service degraded (response time 100x slower)
- CPU/Memory near saturation ‚Üí bottleneck
- Users experiencing slowness (10 sec intolerable)

**kubectl limitations:**
- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç container status (alive/dead)
- –ù–ï –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç service health (fast/slow)
- –ù–ï –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç resource saturation
- –ù–ï –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç trends (—É—Ö—É–¥—à–∞–µ—Ç—Å—è –ª–∏?)

**Monitoring –≤–∏–¥–∏—Ç:**
- Response time –≥—Ä–∞—Ñ–∏–∫: –±—ã–ª 100ms, —Å—Ç–∞–ª 10s (—Ä–µ–∑–∫–∏–π —Å–∫–∞—á–æ–∫)
- CPU usage: –ø–æ—Å—Ç–æ—è–Ω–Ω–æ 95% (saturation)
- Alert fires: "Response time > 1s for 5 minutes"

**–í—ã–≤–æ–¥:** kubectl –≥–æ–≤–æ—Ä–∏—Ç "–≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç". Monitoring –≥–æ–≤–æ—Ä–∏—Ç "—Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –ø–ª–æ—Ö–æ". –î–ª—è production –Ω—É–∂–µ–Ω monitoring, –Ω–µ —Ç–æ–ª—å–∫–æ kubectl.

</details>

---

## üîÑ –¶–ò–ö–õ 2: PROMETHEUS ARCHITECTURE (12 –º–∏–Ω—É—Ç)

### üé¨ –°—é–∂–µ—Ç (2 –º–∏–Ω)

**–î–µ–Ω—å 51, 10:30 ‚Äî Prometheus installation –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è**

**Gu√∞r√∫n:** "First ‚Äî understand how Prometheus works. Not magic. Simple design: pull metrics from targets, store in database, allow queries. Let me show architecture."

*[Gu√∞r√∫n —Ä–∏—Å—É–µ—Ç –Ω–∞ whiteboard –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Prometheus]*

**Gu√∞r√∫n:** "Prometheus = time-series database. Every metric ‚Äî series of (timestamp, value) pairs. Example: CPU usage at 10:00 = 45%, at 10:01 = 47%, at 10:02 = 50%. Time-series. Prometheus stores millions of these."

**–ú–∞–∫—Å:** "–ê –æ—Ç–∫—É–¥–∞ Prometheus –ø–æ–ª—É—á–∞–µ—Ç metrics? –ö–∞–∂–¥–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å?"

**Gu√∞r√∫n:** "No. Prometheus **pulls** metrics (scraping model). Your application exposes /metrics endpoint. Prometheus regularly fetches it. This is different from push-based systems like StatsD. Pull = simpler, more reliable. Target down? Prometheus knows immediately ‚Äî scrape fails."

### üìö –¢–µ–æ—Ä–∏—è: Prometheus Deep Dive (7-10 –º–∏–Ω)

**–ú–µ—Ç–∞—Ñ–æ—Ä–∞: Prometheus = –í—Ä–∞—á —Å –æ–±—Ö–æ–¥–æ–º –ø–∞–ª–∞—Ç**

–ü—Ä–µ–¥—Å—Ç–∞–≤—å –±–æ–ª—å–Ω–∏—Ü—É:
- **–ü–∞—Ü–∏–µ–Ω—Ç—ã** = Targets (Kubernetes pods, servers)
- **–í—Ä–∞—á** = Prometheus (–¥–µ–ª–∞–µ—Ç –æ–±—Ö–æ–¥)
- **–ò–∑–º–µ—Ä–µ–Ω–∏—è** = Metrics (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, –ø—É–ª—å—Å, –¥–∞–≤–ª–µ–Ω–∏–µ)
- **–ö–∞—Ä—Ç—ã –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤** = Time-series database
- **–ú–µ–¥—Å–µ—Å—Ç—Ä–∞** = Alertmanager (–≤—ã–∑—ã–≤–∞–µ—Ç –≤—Ä–∞—á–∞ –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞)

–í—Ä–∞—á –Ω–µ –∂–¥—ë—Ç –ø–æ–∫–∞ –ø–∞—Ü–∏–µ–Ω—Ç—ã –ø–æ–∑–≤–æ–Ω—è—Ç. –û–Ω —Ä–µ–≥—É–ª—è—Ä–Ω–æ –æ–±—Ö–æ–¥–∏—Ç –ø–∞–ª–∞—Ç—ã –∏ –∏–∑–º–µ—Ä—è–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏. –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ —Ç–∞–∫ ‚Äî —Å—Ä–∞–∑—É –≤–∏–¥–∏—Ç –∏ —Ä–µ–∞–≥–∏—Ä—É–µ—Ç.

---

### **Prometheus Components:**

#### 1. **Prometheus Server** (core):

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PROMETHEUS SERVER                  ‚îÇ
‚îÇ                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Retrieval ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Time-series DB  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (scraper) ‚îÇ      ‚îÇ   (TSDB)        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ        ‚Üì                      ‚Üë            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Service   ‚îÇ      ‚îÇ   HTTP Server   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Discovery  ‚îÇ      ‚îÇ  (API + Web UI) ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ    Rules Engine (alerts)           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Retrieval (scraper):**
- Pulls metrics from `/metrics` endpoints
- Default interval: 15s (configurable)
- Parallel scraping (hundreds of targets simultaneously)
- Timeout: 10s default

**Time-series DB (TSDB):**
- Compressed storage (1 byte per sample typical)
- Retention: 15 days default (configurable)
- Efficient –¥–ª—è millions of time-series
- Stored on disk (persistent volume needed)

**HTTP Server:**
- Web UI: http://prometheus:9090
- API: http://prometheus:9090/api/v1/query
- Query PromQL interactively

**Rules Engine:**
- Recording rules (pre-compute expensive queries)
- Alerting rules (detect problems, send to Alertmanager)

---

#### 2. **Targets** (what Prometheus monitors):

**Types:**
- **Kubernetes resources:** pods, nodes, services
- **Exporters:** node_exporter (system metrics), blackbox_exporter (endpoint probes)
- **Applications:** your app with `/metrics` endpoint

**Metrics endpoint format (Prometheus exposition format):**

```
# HELP http_requests_total Total HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="GET",status="200"} 1523
http_requests_total{method="GET",status="404"} 42
http_requests_total{method="POST",status="201"} 342

# HELP memory_usage_bytes Current memory usage
# TYPE memory_usage_bytes gauge
memory_usage_bytes 2147483648

# HELP http_request_duration_seconds HTTP request latency
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{le="0.1"} 500
http_request_duration_seconds_bucket{le="0.5"} 800
http_request_duration_seconds_bucket{le="1.0"} 950
http_request_duration_seconds_bucket{le="+Inf"} 1000
http_request_duration_seconds_sum 450.5
http_request_duration_seconds_count 1000
```

---

#### 3. **Service Discovery** (automatic target detection):

–í–º–µ—Å—Ç–æ hardcoded IP addresses:

```yaml
# ‚ùå BAD: manual targets
scrape_configs:
  - job_name: 'my-app'
    static_configs:
      - targets: ['10.0.1.5:8080', '10.0.1.6:8080']
```

–ò—Å–ø–æ–ª—å–∑—É–π service discovery:

```yaml
# ‚úÖ GOOD: automatic discovery
scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
```

Prometheus –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ pods —Å annotation `prometheus.io/scrape: "true"`.

**Supported service discovery:**
- Kubernetes (pods, services, nodes, endpoints)
- Consul, Eureka, Zookeeper
- AWS EC2, Azure, GCP
- File-based (JSON/YAML —Ñ–∞–π–ª —Å targets)

---

#### 4. **Metric Types:**

**Counter** (only goes up):
```
http_requests_total 1000
# ...15 seconds later...
http_requests_total 1050  # +50 requests
```
Use cases: requests count, errors count, bytes sent

**Gauge** (can go up and down):
```
memory_usage_bytes 2147483648
# ...later...
memory_usage_bytes 2047483648  # decreased
```
Use cases: CPU %, memory usage, temperature, concurrent requests

**Histogram** (distribution of values):
```
http_request_duration_seconds_bucket{le="0.1"} 500   # 500 requests < 100ms
http_request_duration_seconds_bucket{le="0.5"} 800   # 800 requests < 500ms
http_request_duration_seconds_bucket{le="1.0"} 950   # 950 requests < 1s
```
Use cases: latency, request sizes

**Summary** (like histogram but client-side quantiles):
```
http_request_duration_seconds{quantile="0.5"} 0.15   # Median: 150ms
http_request_duration_seconds{quantile="0.95"} 0.45  # p95: 450ms
http_request_duration_seconds{quantile="0.99"} 0.80  # p99: 800ms
```
Use cases: latency percentiles

---

#### 5. **Labels** (dimensions –¥–ª—è filtering):

Metrics —Å labels –ø–æ–∑–≤–æ–ª—è—é—Ç slice and dice data:

```promql
# Total requests:
sum(http_requests_total)

# Requests —Ç–æ–ª—å–∫–æ GET:
sum(http_requests_total{method="GET"})

# Requests GET + status 200:
sum(http_requests_total{method="GET",status="200"})

# Requests by method (–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞):
sum by (method) (http_requests_total)
```

**Best practices –¥–ª—è labels:**
- ‚úÖ Low cardinality (method: GET/POST/PUT/DELETE ‚Äî OK)
- ‚ùå High cardinality (user_id label ‚Äî BAD! millions of users = millions of time-series)
- ‚úÖ Filterable dimensions (status code, method, endpoint)
- ‚ùå Dynamic values (timestamps, UUIDs)

---

**Prometheus Configuration:**

`prometheus.yml` (main config file):

```yaml
global:
  scrape_interval: 15s       # How often to scrape targets
  evaluation_interval: 15s   # How often to evaluate rules
  scrape_timeout: 10s        # Scrape timeout

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

# Load alerting rules
rule_files:
  - "alerts.yml"

# Scrape configurations
scrape_configs:
  # Job 1: Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Job 2: Kubernetes nodes
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:9100'
        target_label: __address__

  # Job 3: Kubernetes pods (auto-discovery)
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
```

---

**Gu√∞r√∫n's wisdom:**
> "Prometheus is pull-based for reason. Push systems ‚Äî targets must know where to send. Pull ‚Äî Prometheus controls scraping. Target down? Prometheus sees immediately (scrape fails). Target up? Prometheus discovers automatically. Simple, reliable, scalable."

**LILITH:**
> "Prometheus architecture ‚Äî —ç–ª–µ–≥–∞–Ω—Ç–Ω–∞—è –ø—Ä–æ—Å—Ç–æ—Ç–∞. Pull metrics, store time-series, query —Å PromQL. –ù–µ –ø—ã—Ç–∞–µ—Ç—Å—è –±—ã—Ç—å –≤—Å–µ–º –¥–ª—è –≤—Å–µ—Ö. Metrics only. Logs ‚Äî Loki. Traces ‚Äî Jaeger. Focused tool does one thing well. Unix philosophy –≤ –º–∏—Ä–µ monitoring."

### üíª –ü—Ä–∞–∫—Ç–∏–∫–∞ (3-5 –º–∏–Ω)

**–ó–∞–¥–∞–Ω–∏–µ 2: Metric types**

Match metric type to use case:

1. HTTP requests count ‚Üí ?
2. Current memory usage ‚Üí ?
3. Request latency distribution ‚Üí ?
4. Current temperature ‚Üí ?
5. Bytes transferred total ‚Üí ?

<details>
<summary>üí° –î—É–º–∞–π –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π</summary>

**–û—Ç–≤–µ—Ç—ã:**

1. **HTTP requests count** ‚Üí **Counter**
   - Only increases (never decreases)
   - Use `rate()` to get requests/sec

2. **Current memory usage** ‚Üí **Gauge**
   - Can go up and down
   - Use directly in queries

3. **Request latency distribution** ‚Üí **Histogram**
   - Need percentiles (p50, p95, p99)
   - Shows distribution across buckets

4. **Current temperature** ‚Üí **Gauge**
   - Can increase or decrease
   - Snapshot of current value

5. **Bytes transferred total** ‚Üí **Counter**
   - Only increases
   - Use `rate()` to get bytes/sec

**–ü—Ä–∞–≤–∏–ª–æ –±–æ–ª—å—à–æ–≥–æ –ø–∞–ª—å—Ü–∞:**
- Counting events ‚Üí Counter
- Measuring current value ‚Üí Gauge
- Measuring distribution ‚Üí Histogram/Summary

</details>

### ü§î –í–æ–ø—Ä–æ—Å LILITH (1 –º–∏–Ω)

**LILITH:** "–ü–æ—á–µ–º—É Prometheus –∏—Å–ø–æ–ª—å–∑—É–µ—Ç pull model (scraping), –∞ –Ω–µ push model (targets –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç metrics)?"

<details>
<summary>üí° –î—É–º–∞–π –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π</summary>

**–û—Ç–≤–µ—Ç—ã (3 –≥–ª–∞–≤–Ω—ã—Ö –ø—Ä–∏—á–∏–Ω—ã):**

**1. Failure detection:**
- **Pull:** Scrape fails ‚Üí Prometheus —Å—Ä–∞–∑—É –∑–Ω–∞–µ—Ç target down
- **Push:** Target down ‚Üí –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö (–º–æ–∂–µ—Ç –±—ã—Ç—å network issue –∏–ª–∏ target dead, –Ω–µ—è—Å–Ω–æ)

**2. Centralized control:**
- **Pull:** Prometheus –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç scraping (interval, timeout, targets)
- **Push:** Targets –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—Ç (–º–æ–≥—É—Ç overwhelm collector, DDoS risk)

**3. Service discovery:**
- **Pull:** Prometheus –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç targets (Kubernetes service discovery)
- **Push:** Targets –¥–æ–ª–∂–Ω—ã –∑–Ω–∞—Ç—å –∫—É–¥–∞ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å (configuration propagation problem)

**4. Debugging (bonus):**
- **Pull:** –ú–æ–∂–Ω–æ –≤—Ä—É—á–Ω—É—é curl target `/metrics` endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
- **Push:** –ù—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø –∫ target –¥–ª—è debugging

**–ö–æ–≥–¥–∞ push –ª—É—á—à–µ:**
- Short-lived jobs (batch jobs, serverless) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π Pushgateway
- Firewall –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è (target –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –∏–∑–≤–Ω–µ) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π agent

**–í—ã–≤–æ–¥:** Pull = default choice –¥–ª—è long-running services. Push = edge cases only.

</details>

---

## üîÑ –¶–ò–ö–õ 3: PROMETHEUS INSTALLATION (15 –º–∏–Ω—É—Ç)

### üé¨ –°—é–∂–µ—Ç (2 –º–∏–Ω)

**–î–µ–Ω—å 51, 11:30 ‚Äî –ü—Ä–∞–∫—Ç–∏–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è**

**Gu√∞r√∫n:** "Theory enough. Time for practice. We deploy Prometheus on your Kubernetes cluster. I prepared manifests. But you apply them ‚Äî learn by doing."

**–ú–∞–∫—Å:** "Prometheus –∫–∞–∫ –æ–±—ã—á–Ω—ã–π pod –≤ Kubernetes?"

**Gu√∞r√∫n:** "Yes. StatefulSet for Prometheus (needs persistent storage for TSDB). ConfigMap –¥–ª—è prometheus.yml. Service –¥–ª—è access. Standard Kubernetes patterns. Prometheus designed for containerized environments from start."

**Gu√∞r√∫n:** "After deployment ‚Äî Prometheus automatically discovers your Kubernetes pods, nodes, services. No manual configuration. Magic of service discovery."

### üìö –¢–µ–æ—Ä–∏—è: Prometheus –Ω–∞ Kubernetes (7-10 –º–∏–Ω)

**Deployment Options:**

#### Option 1: Prometheus Operator (recommended –¥–ª—è production)
- CRDs: ServiceMonitor, PodMonitor, PrometheusRule
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- –°–ª–æ–∂–Ω–µ–µ –¥–ª—è –Ω–∞—á–∞–ª–∞

#### Option 2: Helm Chart
- –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
- Pre-configured –¥–ª—è Kubernetes
- –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è Episode 26

#### Option 3: Manual manifests (educational)
- –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
- –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞

**–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º Helm** (–±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –ø—Ä–æ—Å—Ç–æ—Ç–æ–π –∏ production-readiness).

### Prometheus Stack Components:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PROMETHEUS STACK                     ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Prometheus  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Alertmanager  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Server    ‚îÇ      ‚îÇ                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ         ‚Üì                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Grafana   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Prometheus    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Dashboards  ‚îÇ      ‚îÇ (data source)  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ node-       ‚îÇ      ‚îÇ  kube-state-   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ exporter    ‚îÇ      ‚îÇ  metrics       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Components:**

1. **Prometheus Server** ‚Äî core (scraping, storage, queries)
2. **Alertmanager** ‚Äî alert handling
3. **Grafana** ‚Äî visualization
4. **node-exporter** ‚Äî system metrics (CPU, Memory, Disk)
5. **kube-state-metrics** ‚Äî Kubernetes state metrics (deployments, pods status)

### Installation Steps:

```bash
# 1. Add Helm repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# 2. Create namespace
kubectl create namespace monitoring

# 3. Install Prometheus stack
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set prometheus.prometheusSpec.retention=15d \
  --set grafana.adminPassword=admin

# 4. Verify installation
kubectl get pods -n monitoring
```

**Installed resources:**

```
NAME                                                     READY   STATUS
prometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running
prometheus-grafana-xxxxx                                 3/3     Running
prometheus-kube-state-metrics-xxxxx                      1/1     Running
prometheus-prometheus-node-exporter-xxxxx                1/1     Running
alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running
prometheus-kube-prometheus-operator-xxxxx                1/1     Running
```

**Access URLs:**

```bash
# Prometheus UI
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# Open: http://localhost:9090

# Grafana UI
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
# Open: http://localhost:3000
# Login: admin / admin

# Alertmanager UI
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-alertmanager 9093:9093
# Open: http://localhost:9093
```

**LILITH:**
> "Helm chart —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å—ë –∑–∞ –æ–¥–Ω—É –∫–æ–º–∞–Ω–¥—É. Production-ready setup. –ù–æ –ø–æ–Ω–∏–º–∞–π —á—Ç–æ –≤–Ω—É—Ç—Ä–∏: Prometheus StatefulSet (persistent storage), ConfigMap (prometheus.yml), Service (exposure), ServiceAccount (RBAC permissions). Helm —É–¥–æ–±–µ–Ω, –Ω–æ –Ω–µ magic. –í—Å—ë ‚Äî Kubernetes resources."

### üíª –ü—Ä–∞–∫—Ç–∏–∫–∞ (3-5 –º–∏–Ω)

**–ó–∞–¥–∞–Ω–∏–µ 3: Install Prometheus Stack**

```bash
# 1. Add Helm repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# 2. Create monitoring namespace
kubectl create namespace monitoring

# 3. Install stack
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set prometheus.prometheusSpec.retention=15d \
  --set grafana.adminPassword=admin123

# 4. Wait for pods ready
kubectl wait --for=condition=Ready pods --all -n monitoring --timeout=300s

# 5. Port-forward Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090 &

# 6. Port-forward Grafana
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 &

# 7. Open browser
echo "Prometheus: http://localhost:9090"
echo "Grafana: http://localhost:3000 (admin/admin123)"
```

### ü§î –í–æ–ø—Ä–æ—Å LILITH (1 –º–∏–Ω)

**LILITH:** "–ü–æ—á–µ–º—É Prometheus –∏—Å–ø–æ–ª—å–∑—É–µ—Ç StatefulSet, –∞ –Ω–µ Deployment?"

<details>
<summary>üí° –î—É–º–∞–π –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π</summary>

**–û—Ç–≤–µ—Ç:** **Persistent storage + stable identity.**

**StatefulSet provides:**
1. **Persistent Volume** ‚Äî TSDB data —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ø—Ä–∏ restart
2. **Stable network identity** ‚Äî prometheus-0 (–Ω–µ random pod name)
3. **Ordered deployment** ‚Äî guaranteed startup order
4. **Controlled updates** ‚Äî rolling update —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º

**–ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ:**
- Prometheus —Ö—Ä–∞–Ω–∏—Ç time-series data –Ω–∞ –¥–∏—Å–∫–µ (retention 15 –¥–Ω–µ–π)
- Pod restart ‚Üí data –¥–æ–ª–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å—Å—è (PersistentVolume)
- Deployment –±—ã —Å–æ–∑–¥–∞–ª –Ω–æ–≤—ã–π pod —Å empty disk (data loss!)

**Kubernetes patterns:**
- **Stateless apps** (nginx, API) ‚Üí Deployment
- **Stateful apps** (databases, Prometheus) ‚Üí StatefulSet

</details>

---

## üîÑ –¶–ò–ö–õ 4: PROMQL BASICS (12 –º–∏–Ω—É—Ç)

### üé¨ –°—é–∂–µ—Ç (2 –º–∏–Ω)

**–î–µ–Ω—å 51, 13:00 ‚Äî Prometheus UI**

**Gu√∞r√∫n:** "Prometheus installed. Now ‚Äî query language. PromQL. Like SQL for metrics. You ask question ‚Äî Prometheus answers with numbers."

*[Gu√∞r√∫n opens Prometheus UI: http://localhost:9090]*

**Gu√∞r√∫n:** "Simple query: `up`. Shows which targets are up (1) or down (0). Try it."

*[–ú–∞–∫—Å –≤–≤–æ–¥–∏—Ç query]*

**Gu√∞r√∫n:** "Good. Now complex query: `rate(http_requests_total[5m])`. Shows requests per second over last 5 minutes. PromQL powerful but not intuitive. I teach you patterns."

### üìö –¢–µ–æ—Ä–∏—è: PromQL Query Language (6-8 –º–∏–Ω)

**PromQL = SQL –¥–ª—è time-series**

| SQL | PromQL |
|-----|--------|
| SELECT * FROM table | metric_name |
| WHERE status='200' | metric{status="200"} |
| COUNT(*) | sum(metric) |
| AVG(value) | avg(metric) |
| GROUP BY method | sum by (method) (metric) |

**Basic Queries:**

```promql
# 1. Instant vector (current value)
up  # Is target up? 1 = yes, 0 = no

# 2. Filter by labels
up{job="kubernetes-nodes"}  # Only nodes

# 3. Range vector (last 5 minutes)
http_requests_total[5m]  # All values from last 5m

# 4. Rate (per-second rate)
rate(http_requests_total[5m])  # Requests/sec

# 5. Sum aggregation
sum(rate(http_requests_total[5m]))  # Total req/sec

# 6. Group by label
sum by (method) (rate(http_requests_total[5m]))  # Per HTTP method
```

**Common Functions:**

```promql
# Rate (for counters)
rate(metric[5m])  # Per-second rate

# irate (instant rate, –±–æ–ª–µ–µ responsive)
irate(metric[5m])  # Instant rate

# Increase (total increase)
increase(metric[5m])  # Total count increase

# Avg/Min/Max
avg(metric)
min(metric)
max(metric)

# Aggregations
sum(metric)          # Total
avg(metric)          # Average
count(metric)        # Count of time-series
topk(5, metric)      # Top 5 values
bottomk(3, metric)   # Bottom 3 values
```

**Example Queries –¥–ª—è Kubernetes:**

```promql
# CPU usage per pod
sum by (pod) (rate(container_cpu_usage_seconds_total[5m]))

# Memory usage
container_memory_usage_bytes{namespace="shadow-ops"}

# Pod restarts
kube_pod_container_status_restarts_total

# HTTP request rate
rate(http_requests_total{status="200"}[5m])

# Error rate (%)
sum(rate(http_requests_total{status=~"5.."}[5m])) / 
sum(rate(http_requests_total[5m])) * 100
```

**LILITH:**
> "PromQL —Å–Ω–∞—á–∞–ª–∞ –∫–∞–∂–µ—Ç—Å—è —Å—Ç—Ä–∞–Ω–Ω—ã–º. rate([5m]), sum by (), {} vs []. –ù–æ –ª–æ–≥–∏–∫–∞ –ø—Ä–æ—Å—Ç–∞—è: metric ‚Äî —á—Ç–æ, {labels} ‚Äî —Ñ–∏–ª—å—Ç—Ä, [time] ‚Äî –æ–∫–Ω–æ, function ‚Äî –∞–≥—Ä–µ–≥–∞—Ü–∏—è. –ó–∞–ø–æ–º–Ω–∏ patterns: rate –¥–ª—è counters, {} –¥–ª—è filter, by () –¥–ª—è group. –û—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äî –ø—Ä–∞–∫—Ç–∏–∫–∞."

### üíª –ü—Ä–∞–∫—Ç–∏–∫–∞ (3 –º–∏–Ω)

Open Prometheus UI (http://localhost:9090) and try:

```promql
# 1. Check targets status
up

# 2. CPU usage
rate(node_cpu_seconds_total[5m])

# 3. Memory usage
node_memory_MemAvailable_bytes

# 4. Kubernetes pods count
count(kube_pod_info)

# 5. Pod CPU by namespace
sum by (namespace) (rate(container_cpu_usage_seconds_total[5m]))
```

---

## üîÑ –¶–ò–ö–õ 5: GRAFANA DASHBOARDS (12 –º–∏–Ω—É—Ç)

### üé¨ –°—é–∂–µ—Ç (2 –º–∏–Ω)

**–î–µ–Ω—å 51, 14:00 ‚Äî Grafana UI**

**Gu√∞r√∫n:** "Prometheus queries work. But staring at numbers ‚Äî not comfortable. Grafana ‚Äî visualization. Graphs, charts, alerts. Beautiful dashboards. I show you."

*[Opens Grafana: http://localhost:3000, login: admin/admin123]*

**Gu√∞r√∫n:** "Pre-installed dashboards from kube-prometheus-stack. Kubernetes cluster overview, node metrics, pod resources. Everything out of box. But you learn to create custom dashboard. This is real skill."

### üìö –¢–µ–æ—Ä–∏—è: Grafana Dashboards (6-8 –º–∏–Ω)

**Dashboard Structure:**
```
Dashboard (collection of panels)
  ‚îú‚îÄ Row 1: System Overview
  ‚îÇ   ‚îú‚îÄ Panel 1: CPU Usage (graph)
  ‚îÇ   ‚îî‚îÄ Panel 2: Memory Usage (graph)
  ‚îú‚îÄ Row 2: Network
  ‚îÇ   ‚îú‚îÄ Panel 3: Network Traffic (graph)
  ‚îÇ   ‚îî‚îÄ Panel 4: Connections (stat)
  ‚îî‚îÄ Row 3: Alerts
      ‚îî‚îÄ Panel 5: Alert list
```

**Panel Types:**
- **Graph:** Time-series line chart
- **Stat:** Single number with trend
- **Gauge:** Progress bar/gauge
- **Table:** Data in table format
- **Heatmap:** Density visualization
- **Alert list:** Active alerts

**Creating Dashboard:**

1. Add data source (Prometheus already configured)
2. Create dashboard
3. Add panel
4. Write PromQL query
5. Configure visualization
6. Save dashboard

**Example Panel ‚Äî CPU Usage:**

```
Query: rate(node_cpu_seconds_total{mode!="idle"}[5m]) * 100
Legend: {{instance}} - {{mode}}
Unit: percent (0-100)
Visualization: Time series
```

**Variables (templating):**

```
Variable: $namespace
Query: label_values(kube_pod_info, namespace)
Usage in query: container_memory_usage_bytes{namespace="$namespace"}
```

–ü–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É namespaces –≤ dropdown.

**LILITH:**
> "Grafana ‚Äî Photoshop –¥–ª—è –º–µ—Ç—Ä–∏–∫. PromQL –¥–∞—ë—Ç –¥–∞–Ω–Ω—ã–µ, Grafana –¥–µ–ª–∞–µ—Ç –∏—Ö –∫—Ä–∞—Å–∏–≤—ã–º–∏. Pre-built dashboards —Ö–æ—Ä–æ—à–∏ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞. Custom dashboards ‚Äî –¥–ª—è production. –§–æ–∫—É—Å–∏—Ä—É–π –Ω–∞ actionable metrics: —á—Ç–æ broken? —á—Ç–æ bottleneck? –Ω–µ vanity metrics."

### üíª –ü—Ä–∞–∫—Ç–∏–∫–∞ (3 –º–∏–Ω)

**–ó–∞–¥–∞–Ω–∏–µ: Create simple dashboard**

1. Open Grafana ‚Üí Dashboards ‚Üí New Dashboard
2. Add panel ‚Üí Prometheus query:
   ```promql
   sum(rate(container_cpu_usage_seconds_total{namespace="shadow-ops"}[5m])) by (pod)
   ```
3. Panel title: "CPU Usage by Pod"
4. Visualization: Time series
5. Legend: {{pod}}
6. Save dashboard: "Shadow Ops Monitoring"

---

## üîÑ –¶–ò–ö–õ 6: ALERTMANAGER & ALERTING RULES (15 –º–∏–Ω—É—Ç)

### üé¨ –°—é–∂–µ—Ç (2 –º–∏–Ω)

**–î–µ–Ω—å 51, 15:30 ‚Äî Alert testing**

**Gu√∞r√∫n:** "Dashboards pretty. But you –Ω–µ –º–æ–∂–µ—à—å watch screens 24/7. Alerts ‚Äî automatic notifications. Prometheus detects problem, Alertmanager sends alert. Email, Slack, PagerDuty. Whatever you need."

*[Gu√∞r√∫n –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç alert firing]*

**Gu√∞r√∫n:** "This alert: 'Pod –Ω–µ Ready more than 5 minutes'. Fires when problem. Resolves when fixed. Self-healing notifications. Production requires this."

### üìö –¢–µ–æ—Ä–∏—è: Alerting (7-10 –º–∏–Ω)

**Alerting Rule Example:**

```yaml
# prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: shadow-ops-alerts
  namespace: monitoring
spec:
  groups:
  - name: shadow-ops
    interval: 30s
    rules:
    # Alert 1: High CPU
    - alert: HighCPU
      expr: sum by (pod) (rate(container_cpu_usage_seconds_total{namespace="shadow-ops"}[5m])) > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on pod {{ $labels.pod }}"
        description: "CPU usage > 90% for 5 minutes"
    
    # Alert 2: Pod not Ready
    - alert: PodNotReady
      expr: kube_pod_status_phase{namespace="shadow-ops",phase!="Running"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod {{ $labels.pod }} not Ready"
        description: "Pod in {{ $labels.phase }} phase for 5 minutes"
```

**Alert States:**
- **Inactive:** Condition false (no problem)
- **Pending:** Condition true, waiting `for` duration
- **Firing:** Condition true for `for` duration ‚Üí alert sent

**Alertmanager Config:**

```yaml
# alertmanager-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-prometheus-kube-prometheus-alertmanager
  namespace: monitoring
stringData:
  alertmanager.yaml: |
    global:
      slack_api_url: 'https://hooks.slack.com/services/xxx'
    
    route:
      receiver: 'slack-notifications'
      group_by: ['alertname', 'cluster']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
    
    receivers:
    - name: 'slack-notifications'
      slack_configs:
      - channel: '#alerts'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
```

**Best Practices:**
- ‚úÖ Alert –Ω–∞ symptoms (high latency) –Ω–µ causes (high CPU)
- ‚úÖ Include runbooks –≤ annotations
- ‚úÖ Use severity levels (critical, warning, info)
- ‚úÖ Group related alerts (–Ω–µ spam 100 alerts)
- ‚ùå Don't alert –Ω–∞ everything (alert fatigue)

---

## üîÑ –¶–ò–ö–õ 7: KUBERNETES MONITORING (12 –º–∏–Ω—É—Ç)

### üìö –¢–µ–æ—Ä–∏—è: Monitoring K8s (—Å–æ–∫—Ä–∞—â—ë–Ω–Ω–æ)

**Key Metrics:**

```promql
# Cluster health
up{job="kubernetes-nodes"}

# Pod status
kube_pod_status_phase

# Resource usage
container_cpu_usage_seconds_total
container_memory_usage_bytes

# Deployment status
kube_deployment_status_replicas_available
```

**ServiceMonitor CRD** (auto-discovery):

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: shadow-web-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: shadow-web
  endpoints:
  - port: metrics
    interval: 30s
```

---

## üîÑ –¶–ò–ö–õ 8: TROUBLESHOOTING & BEST PRACTICES (12 –º–∏–Ω—É—Ç)

### üìö –¢–µ–æ—Ä–∏—è: Common Issues (—Å–æ–∫—Ä–∞—â—ë–Ω–Ω–æ)

**Issue 1: No Data in Grafana**
- Check: Prometheus targets (Status ‚Üí Targets)
- Check: Prometheus data source –≤ Grafana
- Check: PromQL query syntax

**Issue 2: Alert –Ω–µ fires**
- Check: Alerting rules loaded (Status ‚Üí Rules)
- Check: Alert condition true (Graph ‚Üí Expression)
- Check: `for` duration (pending ‚Üí firing time)

**Issue 3: High Cardinality**
- Problem: Too many unique label combinations
- Symptom: Prometheus OOMKilled, slow queries
- Solution: Reduce labels (remove user_id, request_id)

**Best Practices:**
- ‚úÖ Monitor SLIs (Service Level Indicators): latency, availability, error rate
- ‚úÖ Define SLOs (Service Level Objectives): 99.9% uptime, p95 < 500ms
- ‚úÖ Alert –Ω–∞ SLO violations
- ‚úÖ Use dashboards –¥–ª—è exploration, alerts –¥–ª—è action
- ‚úÖ Retention: 15-30 days (longer = more storage)

---

## üìã –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –ó–ê–î–ê–ù–ò–Ø

### –ó–∞–¥–∞–Ω–∏–µ 1: Install Prometheus Stack ‚úÖ (–≤—ã–ø–æ–ª–Ω–µ–Ω–æ –≤ –¶–∏–∫–ª–µ 3)

### –ó–∞–¥–∞–Ω–∏–µ 2: Create Custom Dashboard

1. Login to Grafana
2. Create new dashboard
3. Add panels:
   - CPU usage by pod
   - Memory usage by pod
   - Network traffic
   - Pod status
4. Add variables: $namespace
5. Save dashboard

### –ó–∞–¥–∞–Ω–∏–µ 3: Configure Alert Rule

Create `custom-alerts.yaml`:

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: custom-alerts
  namespace: monitoring
spec:
  groups:
  - name: custom
    rules:
    - alert: HighMemory
      expr: container_memory_usage_bytes{namespace="shadow-ops"} / container_spec_memory_limit_bytes > 0.9
      for: 5m
      annotations:
        summary: "High memory usage"
```

Apply: `kubectl apply -f custom-alerts.yaml`

### –ó–∞–¥–∞–Ω–∏–µ 4: Test Alert

1. Generate high CPU: `kubectl run stress --image=polinux/stress -- stress --cpu 4`
2. Wait 5 minutes
3. Check Alertmanager: http://localhost:9093
4. Verify alert fires
5. Delete stress pod: `kubectl delete pod stress`
6. Verify alert resolves

---

## üé¨ EPILOGUE: –ì–õ–ê–ó–ê –û–¢–ö–†–´–¢–´

### –î–µ–Ω—å 52, 17:00 ‚Äî Monitoring Dashboard

**Gu√∞r√∫n:** "Good work. Prometheus collects. Grafana visualizes. Alertmanager notifies. You have eyes now. Production infrastructure visible."

*[–ù–∞ –º–æ–Ω–∏—Ç–æ—Ä–µ: 10+ dashboards, all green, metrics flowing]*

**Gu√∞r√∫n:** "Before ‚Äî kubectl get pods. Now ‚Äî real-time visibility. CPU, memory, disk, network, HTTP requests, errors, latency. Everything. And when problem ‚Äî alert fires before users notice."

**–ú–∞–∫—Å:** "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ '—á—Ç–æ —Å–ª–æ–º–∞–ª–æ—Å—å', –Ω–æ –∏ '—á—Ç–æ —Å–∫–æ—Ä–æ —Å–ª–æ–º–∞–µ—Ç—Å—è'. –¢—Ä–µ–Ω–¥—ã."

**Gu√∞r√∫n:** "Exactly. This is observability. Not just 'is it up?' but 'how is it performing?'. Production requires this. Without monitoring ‚Äî you are blind. With monitoring ‚Äî you see everything."

**Viktor** (–≤–∏–¥–µ–æ–∑–≤–æ–Ω–æ–∫): "–ú–∞–∫—Å, Episode 26 complete. Monitoring –∞–∫—Ç–∏–≤–µ–Ω. Prometheus scraping, Grafana dashboards, Alertmanager configured. Real-time visibility. Tomorrow ‚Äî Episode 27. √ìlafur teaches performance tuning. Monitoring shows problems. Performance tuning fixes them."

**LILITH:** "–û—Ç —Å–ª–µ–ø–æ–≥–æ kubectl –∫ –ø–æ–ª–Ω–æ–π observability. Prometheus –≤–∏–¥–∏—Ç –∫–∞–∂–¥—É—é –º–µ—Ç—Ä–∏–∫—É. Grafana –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥—ã. Alertmanager –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ—Ç –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö. Production infrastructure –≥–æ—Ç–æ–≤–∞ –∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥—É. Episode 27 ‚Äî –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ–≥–æ —á—Ç–æ –≤–∏–¥–∏—à—å."

---

## üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –≠–ü–ò–ó–û–î–ê

**–ß—Ç–æ –≤—ã –∏–∑—É—á–∏–ª–∏:**
- ‚úÖ Observability principles (metrics, logs, traces)
- ‚úÖ Prometheus architecture (scraping, TSDB, PromQL)
- ‚úÖ Prometheus installation (Helm chart, Kubernetes deployment)
- ‚úÖ PromQL queries (rate, sum, aggregations)
- ‚úÖ Grafana dashboards (panels, variables, visualization)
- ‚úÖ Alertmanager (alerting rules, notifications)
- ‚úÖ Kubernetes monitoring (ServiceMonitor, kube-state-metrics)
- ‚úÖ Troubleshooting (no data, alert issues, high cardinality)

**Monitoring stack deployed:**
- 1 Prometheus Server (StatefulSet)
- 1 Grafana instance
- 1 Alertmanager
- N node-exporters (per node)
- 1 kube-state-metrics
- Custom dashboards + alert rules

**Time spent:** 5-6 hours  
**Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ  
**Production readiness:** 70% (monitoring active, needs alert tuning)

---

## üîó –°–õ–ï–î–£–Æ–©–ò–ô –≠–ü–ò–ó–û–î

**Episode 27: Performance Tuning**
- Performance profiling (perf, top, iostat)
- Kernel tuning (sysctl optimization)
- Database optimization (SQL queries, indexes)
- Caching strategies (Redis)
- **–ü–µ—Ä—Å–æ–Ω–∞–∂:** √ìlafur √û√≥rsson (performance engineer)

**–î–µ–Ω—å 53 –æ–ø–µ—Ä–∞—Ü–∏–∏** ‚Äî 7 –¥–Ω–µ–π –¥–æ —Ñ–∏–Ω–∞–ª–∞

---

<div align="center">

**"Without monitoring ‚Äî you are blind. With monitoring ‚Äî you have eyes."** ‚Äî Gu√∞r√∫n √Åsta

**EPISODE 26 COMPLETE** ‚úÖ

*–î–µ–Ω—å 52 –∏–∑ 60 | –†–µ–π–∫—å—è–≤–∏–∫, –ò—Å–ª–∞–Ω–¥–∏—è üáÆüá∏ | Monitoring infrastructure active*

**Next:** Episode 27 ‚Äî Performance Tuning ‚ö°

</div>